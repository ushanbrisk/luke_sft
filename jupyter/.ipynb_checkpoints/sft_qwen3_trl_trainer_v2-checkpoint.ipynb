{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75b09255-c46f-42b2-9e92-5e64861efa43",
   "metadata": {},
   "source": [
    "# Qwen3微调实战：医疗R1推理风格聊天\n",
    "\n",
    "[![](https://raw.githubusercontent.com/SwanHubX/assets/main/badge1.svg)](https://swanlab.cn/@ZeyiLin/qwen3-sft-medical/overview)\n",
    "\n",
    "- **Github**: [Qwen3-Medical-SFT](https://github.com/Zeyi-Lin/Qwen3-Medical-SFT)\n",
    "- **基础模型**：[Qwen3-1.7B](https://modelscope.cn/models/Qwen/Qwen3-1.7B/summary)\n",
    "- **微调后模型**：[Qwen3-1.7b-Medical-R1-sft](https://modelscope.cn/models/testUser/Qwen3-1.7b-Medical-R1-sft/summary)\n",
    "- **数据集**：[delicate_medical_r1_data](https://modelscope.cn/datasets/krisfu/delicate_medical_r1_data)\n",
    "- **SwanLab**：[qwen3-sft-medical](https://swanlab.cn/@ZeyiLin/qwen3-sft-medical/runs/agps0dkifth5l1xytcdyk/chart)\n",
    "- **微调方式**：全参数微调、LoRA微调\n",
    "- **推理风格**：R1推理风格\n",
    "- **算力要求**：\n",
    "  - **全参数微调**：32GB显存\n",
    "  - **LoRA微调**：28GB显存\n",
    "- **图文教程**：[Qwen3大模型微调入门实战（完整代码）](https://zhuanlan.zhihu.com/p/1903848838214705484)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b4fa4a-03a4-4bdb-b593-b50d2ebbb931",
   "metadata": {},
   "source": [
    "## 1. 安装环境"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac44ba5-45d2-4e43-a906-2e4421b72114",
   "metadata": {},
   "source": [
    "## 3. 登录SwanLab\n",
    "1. 前往[swanlab](https://swanlab.cn/space/~/settings)复制你的API Key，粘贴到下面的代码中\n",
    "2. 如果你不希望将登录信息保存到该计算机中，可将`save=True`去掉（每次运行训练需要重新执行下面的代码块）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76c50f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef1ddf3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49b460a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d5cbc21-ae61-40fd-ae38-843b774fe82c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luke/miniconda3/envs/openr1/lib/python3.11/site-packages/swanlab/data/run/metadata/hardware/gpu/nvidia.py:10: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import swanlab\n",
    "\n",
    "swanlab.login(api_key=\"5Q7lBtLI6qBF8OoEu66Lk\", save=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f43e7f4-976b-46ec-ab63-febeac40a3ce",
   "metadata": {},
   "source": [
    "## 4. 开启全参数微调"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16fc9e42",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luke/miniconda3/envs/openr1/lib/python3.11/site-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from modelscope import snapshot_download, AutoTokenizer\n",
    "from transformers import AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForSeq2Seq\n",
    "\n",
    "from transformers import DataCollatorForLanguageModeling, DataCollatorWithPadding\n",
    "\n",
    "\n",
    "import os\n",
    "import swanlab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5edb9c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ['CUDA_VISIBLE_ICES'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78d71b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "os.environ[\"SWANLAB_PROJECT\"]=\"qwen3-sft-medical-v2-0920\"\n",
    "PROMPT = \"你是一个医学专家，你需要根据用户的问题，给出带有思考的回答。\"\n",
    "MAX_LENGTH = 2048\n",
    "\n",
    "# swanlab.config.update({\n",
    "#     \"model\": \"Qwen/Qwen3-1.7B\",\n",
    "#     \"prompt\": PROMPT,\n",
    "#     \"data_max_length\": MAX_LENGTH,\n",
    "#     })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e21e0fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def dataset_jsonl_transfer(origin_path, new_path):\n",
    "    \"\"\"\n",
    "    将原始数据集转换为大模型微调所需数据格式的新数据集\n",
    "    \"\"\"\n",
    "    messages = []\n",
    "\n",
    "    # 读取旧的JSONL文件\n",
    "    with open(origin_path, \"r\") as file:\n",
    "        for line in file:\n",
    "            # 解析每一行的json数据\n",
    "            data = json.loads(line)\n",
    "            input = data[\"question\"]\n",
    "            think = data[\"think\"]\n",
    "            answer = data[\"answer\"]\n",
    "            output = f\"<think>{think}</think> \\n {answer}\"\n",
    "            message = {\n",
    "                \"instruction\": PROMPT,\n",
    "                \"input\": f\"{input}\",\n",
    "                \"output\": output,\n",
    "            }\n",
    "            messages.append(message)\n",
    "\n",
    "    # 保存重构后的JSONL文件\n",
    "    with open(new_path, \"w\", encoding=\"utf-8\") as file:\n",
    "        for message in messages:\n",
    "            file.write(json.dumps(message, ensure_ascii=False) + \"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88df11b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4b88f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_func2(example):\n",
    "    \n",
    "\n",
    "    instruction = example['instruction']\n",
    "\n",
    "    \n",
    "    input1 = example['input']\n",
    "    \n",
    "    output = example['output']\n",
    "\n",
    "    messages = [\n",
    "        {'role': 'system', 'content': f\"{PROMPT}\"},\n",
    "        {'role': 'user', 'content': input1},\n",
    "        {'role': 'assistant', 'content': output},\n",
    "                ]\n",
    "\n",
    "\n",
    "\n",
    "    text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=False,\n",
    "    truncate=True,\n",
    "#     return_tensors='pt',\n",
    "#     enable_thinking=False\n",
    "\n",
    "    )\n",
    "#     example['text'] = text\n",
    "    return {\"text\":text}\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "07aa602b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def process_func(example):\n",
    "    \"\"\"\n",
    "    将数据集进行预处理\n",
    "    \"\"\" \n",
    "    input_ids, attention_mask, labels = [], [], []\n",
    "    instruction = tokenizer(\n",
    "        f\"<|im_start|>system\\n{PROMPT}<|im_end|>\\n<|im_start|>user\\n{example['input']}<|im_end|>\\n<|im_start|>assistant\\n\",\n",
    "        add_special_tokens=False,\n",
    "    )\n",
    "    response = tokenizer(f\"{example['output']}\", add_special_tokens=False)\n",
    "    input_ids = instruction[\"input_ids\"] + response[\"input_ids\"] + [tokenizer.pad_token_id]\n",
    "    attention_mask = (\n",
    "        instruction[\"attention_mask\"] + response[\"attention_mask\"] + [1]\n",
    "    )\n",
    "    labels = [-100] * len(instruction[\"input_ids\"]) + response[\"input_ids\"] + [tokenizer.pad_token_id]\n",
    "    if len(input_ids) > MAX_LENGTH:  # 做一个截断\n",
    "        input_ids = input_ids[:MAX_LENGTH]\n",
    "        attention_mask = attention_mask[:MAX_LENGTH]\n",
    "        labels = labels[:MAX_LENGTH]\n",
    "    return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"labels\": labels}   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "06ff1faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def predict(messages, model, tokenizer):\n",
    "#     device = model.device\n",
    "    device=\"cuda\"\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n",
    "\n",
    "    generated_ids = model.generate(\n",
    "        model_inputs.input_ids,\n",
    "        max_new_tokens=MAX_LENGTH,\n",
    "    )\n",
    "    generated_ids = [\n",
    "        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "\n",
    "    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b817aecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # 在modelscope上下载Qwen模型到本地目录下\n",
    "# model_dir = snapshot_download(\"Qwen/Qwen3-1.7B\", cache_dir=\"./\", revision=\"master\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "11dfc315",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef2886d2ee474fb0905b34aa20d46b97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Transformers加载模型权重\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/ssd3/Qwen3-1.7B\", use_fast=False, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"/ssd3/Qwen3-1.7B\", device_map=\"cuda:0\", torch_dtype=torch.bfloat16)\n",
    "model.enable_input_require_grads()  # 开启梯度检查点时，要执行该方法\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "48ee02c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.to(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f6caa02a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f75e7dd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen3ForCausalLM(\n",
       "  (model): Qwen3Model(\n",
       "    (embed_tokens): Embedding(151936, 2048)\n",
       "    (layers): ModuleList(\n",
       "      (0-27): 28 x Qwen3DecoderLayer(\n",
       "        (self_attn): Qwen3Attention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "        )\n",
       "        (mlp): Qwen3MLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=6144, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=6144, bias=False)\n",
       "          (down_proj): Linear(in_features=6144, out_features=2048, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen3RMSNorm((2048,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen3RMSNorm((2048,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen3RMSNorm((2048,), eps=1e-06)\n",
       "    (rotary_emb): Qwen3RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d53bc79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "988dfa77",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 加载、处理数据集和测试集\n",
    "train_dataset_path = \"train.jsonl\"\n",
    "test_dataset_path = \"val.jsonl\"\n",
    "\n",
    "train_jsonl_new_path = \"train_format.jsonl\"\n",
    "test_jsonl_new_path = \"val_format.jsonl\"\n",
    "\n",
    "if not os.path.exists(train_jsonl_new_path):\n",
    "    dataset_jsonl_transfer(train_dataset_path, train_jsonl_new_path)\n",
    "if not os.path.exists(test_jsonl_new_path):\n",
    "    dataset_jsonl_transfer(test_dataset_path, test_jsonl_new_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "22033dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 得到训练集\n",
    "train_df = pd.read_json(train_jsonl_new_path, lines=True)\n",
    "train_ds = Dataset.from_pandas(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "53ab10ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = process_func2(train_ds[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0c808a01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': '<|im_start|>system\\n你是一个医学专家，你需要根据用户的问题，给出带有思考的回答。<|im_end|>\\n<|im_start|>user\\n医生，我想了解一下如何测定盐酸甲氧明注射液的含量，具体步骤是什么样的？<|im_end|>\\n<|im_start|>assistant\\n<think>\\n嗯，用户问的是如何测定盐酸甲氧明注射液的含量，具体步骤是什么。首先，我需要回忆一下相关的分析方法，特别是药典中常用的方法。盐酸甲氧明属于药物分析中的常见成分，通常可能用紫外分光光度法或者HPLC之类的。\\n\\n首先，用户提到的是紫外-可见分光光度法，这应该是比较直接的方法。那步骤的话，首先应该考虑样品的制备。因为注射液浓度可能较高，需要稀释到合适的浓度范围，使得吸光度在仪器的线性范围内。比如，先取一定量的注射液，然后稀释到某个体积，比如250毫升的量瓶，这样可能是一个步骤。\\n\\n接下来，可能需要进一步稀释，因为第一次稀释后的溶液可能吸光度还是太高或者太低。比如，从第一次稀释后的溶液中再取一部分，比如10毫升，放到另一个量瓶，比如100毫升，再次稀释。这样两次稀释可以得到合适的浓度。\\n\\n然后，测定吸光度的时候，波长的选择很重要。盐酸甲氧明的最大吸收波长应该查文献或者药典，比如问题中的答案提到的是290纳米，这可能来自药典的规定。需要确认这个波长是否正确，以及吸收系数是否准确。比如，吸收系数137是否对应C1H1NO3·HCl的计算，可能需要根据分子量和浓度来计算，但用户可能不需要详细计算，只需要步骤。\\n\\n另外，步骤中的精密量取需要注意，比如使用移液管或者自动进样器，确保量取的准确性。摇匀也是关键步骤，确保溶液均匀，避免局部浓度不均导致误差。\\n\\n可能还需要考虑空白对照，比如用水代替样品，进行同样的操作，以扣除背景吸光度。不过答案中没有提到，可能在标准方法中已经包含，或者步骤简化了。但作为详细步骤，可能需要补充这一点。\\n\\n最后，计算含量的时候，根据吸收系数和吸光度，应用朗伯-比尔定律，A=εcl，其中ε是摩尔吸光系数，但这里可能用的是百分吸收系数（1% 1cm），所以需要确认单位是否正确。例如，吸收系数137可能是指在1%浓度（1g/100ml）下，1cm光程的吸光度，这样计算时需要将浓度转换为相应的单位。\\n\\n可能用户是医生或者药剂师，需要准确的步骤来执行或理解检测流程，所以步骤要清晰，符合药典标准。需要确保步骤中的每个操作都有依据，比如量取的体积、稀释倍数、波长选择等，都符合药典推荐的方法，这样结果才可靠。\\n\\n总结下来，步骤应该是：取样、两次稀释、测定吸光度、计算含量。同时要强调方法的准确性和药典推荐，让用户知道这是标准方法，可信度高。\\n</think>\\n\\n \\n 您好，测定盐酸甲氧明注射液的含量通常采用紫外-可见分光光度法。首先，需要精密量取适量的注射液（大约相当于100毫克盐酸甲氧明），将其置于250毫升的量瓶中，并用水稀释至刻度线，充分摇匀。然后，从这个溶液中精密量取10毫升，放入100毫升的量瓶中，再次用水稀释至刻度线，摇匀。最后，使用紫外-可见分光光度计，在290纳米的波长处测定吸光度，根据C1H1NO3·HCl的吸收系数137来计算盐酸甲氧明的含量。这种方法准确可靠，是药典推荐的测定方法。<|im_end|>\\n'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2822c5e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2684b434",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": train_ds[0]['input']},\n",
    "    {\"role\": \"assistant\", \"content\": train_ds[0]['output']},    \n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=False,\n",
    "    enable_thinking=True # Switches between thinking and non-thinking modes. Default is True.\n",
    ")\n",
    "text\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e0394a61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|im_start|>user\\n医生，我想了解一下如何测定盐酸甲氧明注射液的含量，具体步骤是什么样的？<|im_end|>\\n<|im_start|>assistant\\n<think>\\n嗯，用户问的是如何测定盐酸甲氧明注射液的含量，具体步骤是什么。首先，我需要回忆一下相关的分析方法，特别是药典中常用的方法。盐酸甲氧明属于药物分析中的常见成分，通常可能用紫外分光光度法或者HPLC之类的。\\n\\n首先，用户提到的是紫外-可见分光光度法，这应该是比较直接的方法。那步骤的话，首先应该考虑样品的制备。因为注射液浓度可能较高，需要稀释到合适的浓度范围，使得吸光度在仪器的线性范围内。比如，先取一定量的注射液，然后稀释到某个体积，比如250毫升的量瓶，这样可能是一个步骤。\\n\\n接下来，可能需要进一步稀释，因为第一次稀释后的溶液可能吸光度还是太高或者太低。比如，从第一次稀释后的溶液中再取一部分，比如10毫升，放到另一个量瓶，比如100毫升，再次稀释。这样两次稀释可以得到合适的浓度。\\n\\n然后，测定吸光度的时候，波长的选择很重要。盐酸甲氧明的最大吸收波长应该查文献或者药典，比如问题中的答案提到的是290纳米，这可能来自药典的规定。需要确认这个波长是否正确，以及吸收系数是否准确。比如，吸收系数137是否对应C1H1NO3·HCl的计算，可能需要根据分子量和浓度来计算，但用户可能不需要详细计算，只需要步骤。\\n\\n另外，步骤中的精密量取需要注意，比如使用移液管或者自动进样器，确保量取的准确性。摇匀也是关键步骤，确保溶液均匀，避免局部浓度不均导致误差。\\n\\n可能还需要考虑空白对照，比如用水代替样品，进行同样的操作，以扣除背景吸光度。不过答案中没有提到，可能在标准方法中已经包含，或者步骤简化了。但作为详细步骤，可能需要补充这一点。\\n\\n最后，计算含量的时候，根据吸收系数和吸光度，应用朗伯-比尔定律，A=εcl，其中ε是摩尔吸光系数，但这里可能用的是百分吸收系数（1% 1cm），所以需要确认单位是否正确。例如，吸收系数137可能是指在1%浓度（1g/100ml）下，1cm光程的吸光度，这样计算时需要将浓度转换为相应的单位。\\n\\n可能用户是医生或者药剂师，需要准确的步骤来执行或理解检测流程，所以步骤要清晰，符合药典标准。需要确保步骤中的每个操作都有依据，比如量取的体积、稀释倍数、波长选择等，都符合药典推荐的方法，这样结果才可靠。\\n\\n总结下来，步骤应该是：取样、两次稀释、测定吸光度、计算含量。同时要强调方法的准确性和药典推荐，让用户知道这是标准方法，可信度高。\\n</think>\\n\\n \\n 您好，测定盐酸甲氧明注射液的含量通常采用紫外-可见分光光度法。首先，需要精密量取适量的注射液（大约相当于100毫克盐酸甲氧明），将其置于250毫升的量瓶中，并用水稀释至刻度线，充分摇匀。然后，从这个溶液中精密量取10毫升，放入100毫升的量瓶中，再次用水稀释至刻度线，摇匀。最后，使用紫外-可见分光光度计，在290纳米的波长处测定吸光度，根据C1H1NO3·HCl的吸收系数137来计算盐酸甲氧明的含量。这种方法准确可靠，是药典推荐的测定方法。<|im_end|>\\n'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68e1eef",
   "metadata": {},
   "source": [
    "此处的token, 没有用chat_template模板，而是直接手动拼接，　模板函数的功能，也仅仅是把各个字段封装成 start, end的字段\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7148d857",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1bf3f4550404d2e83801b2ba89f415d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2166 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "train_dataset = train_ds.map(process_func, remove_columns=train_ds.column_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "376e6c99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [151644,\n",
       "  8948,\n",
       "  198,\n",
       "  56568,\n",
       "  101909,\n",
       "  104316,\n",
       "  101057,\n",
       "  3837,\n",
       "  112735,\n",
       "  100345,\n",
       "  20002,\n",
       "  103936,\n",
       "  3837,\n",
       "  107485,\n",
       "  106646,\n",
       "  104107,\n",
       "  111423,\n",
       "  1773,\n",
       "  151645,\n",
       "  198,\n",
       "  151644,\n",
       "  872,\n",
       "  198,\n",
       "  103998,\n",
       "  3837,\n",
       "  104100,\n",
       "  110050,\n",
       "  100007,\n",
       "  115391,\n",
       "  102029,\n",
       "  99918,\n",
       "  100043,\n",
       "  100316,\n",
       "  30858,\n",
       "  107833,\n",
       "  100058,\n",
       "  9370,\n",
       "  104982,\n",
       "  3837,\n",
       "  100398,\n",
       "  105652,\n",
       "  102021,\n",
       "  100535,\n",
       "  11319,\n",
       "  151645,\n",
       "  198,\n",
       "  151644,\n",
       "  77091,\n",
       "  198,\n",
       "  151667,\n",
       "  106287,\n",
       "  3837,\n",
       "  20002,\n",
       "  56007,\n",
       "  100146,\n",
       "  100007,\n",
       "  115391,\n",
       "  102029,\n",
       "  99918,\n",
       "  100043,\n",
       "  100316,\n",
       "  30858,\n",
       "  107833,\n",
       "  100058,\n",
       "  9370,\n",
       "  104982,\n",
       "  3837,\n",
       "  100398,\n",
       "  105652,\n",
       "  102021,\n",
       "  1773,\n",
       "  101140,\n",
       "  3837,\n",
       "  35946,\n",
       "  85106,\n",
       "  104843,\n",
       "  100158,\n",
       "  105470,\n",
       "  101042,\n",
       "  39907,\n",
       "  3837,\n",
       "  104050,\n",
       "  99471,\n",
       "  99548,\n",
       "  15946,\n",
       "  103229,\n",
       "  104339,\n",
       "  1773,\n",
       "  102029,\n",
       "  99918,\n",
       "  100043,\n",
       "  100316,\n",
       "  30858,\n",
       "  100409,\n",
       "  104459,\n",
       "  101042,\n",
       "  101047,\n",
       "  101536,\n",
       "  105024,\n",
       "  3837,\n",
       "  102119,\n",
       "  87267,\n",
       "  11622,\n",
       "  114794,\n",
       "  17177,\n",
       "  99225,\n",
       "  99225,\n",
       "  26381,\n",
       "  24339,\n",
       "  100631,\n",
       "  39,\n",
       "  2916,\n",
       "  34,\n",
       "  106896,\n",
       "  3407,\n",
       "  101140,\n",
       "  3837,\n",
       "  20002,\n",
       "  104496,\n",
       "  100146,\n",
       "  114794,\n",
       "  12,\n",
       "  101479,\n",
       "  17177,\n",
       "  99225,\n",
       "  99225,\n",
       "  26381,\n",
       "  24339,\n",
       "  3837,\n",
       "  43288,\n",
       "  104583,\n",
       "  99792,\n",
       "  101041,\n",
       "  104339,\n",
       "  1773,\n",
       "  99212,\n",
       "  105652,\n",
       "  100363,\n",
       "  3837,\n",
       "  101140,\n",
       "  99730,\n",
       "  101118,\n",
       "  111253,\n",
       "  9370,\n",
       "  43316,\n",
       "  56278,\n",
       "  1773,\n",
       "  99519,\n",
       "  107833,\n",
       "  100058,\n",
       "  108010,\n",
       "  87267,\n",
       "  105540,\n",
       "  3837,\n",
       "  85106,\n",
       "  101474,\n",
       "  68862,\n",
       "  26939,\n",
       "  106873,\n",
       "  108010,\n",
       "  101121,\n",
       "  3837,\n",
       "  104193,\n",
       "  99544,\n",
       "  99225,\n",
       "  26381,\n",
       "  18493,\n",
       "  106550,\n",
       "  9370,\n",
       "  43268,\n",
       "  33071,\n",
       "  104589,\n",
       "  1773,\n",
       "  101912,\n",
       "  3837,\n",
       "  60726,\n",
       "  18158,\n",
       "  99623,\n",
       "  32757,\n",
       "  9370,\n",
       "  107833,\n",
       "  100058,\n",
       "  3837,\n",
       "  101889,\n",
       "  101474,\n",
       "  68862,\n",
       "  26939,\n",
       "  106168,\n",
       "  110238,\n",
       "  3837,\n",
       "  101912,\n",
       "  17,\n",
       "  20,\n",
       "  15,\n",
       "  117033,\n",
       "  9370,\n",
       "  32757,\n",
       "  100822,\n",
       "  3837,\n",
       "  99654,\n",
       "  87267,\n",
       "  101909,\n",
       "  105652,\n",
       "  3407,\n",
       "  104326,\n",
       "  3837,\n",
       "  87267,\n",
       "  85106,\n",
       "  100642,\n",
       "  101474,\n",
       "  68862,\n",
       "  3837,\n",
       "  99519,\n",
       "  104064,\n",
       "  101474,\n",
       "  68862,\n",
       "  104813,\n",
       "  114357,\n",
       "  87267,\n",
       "  99544,\n",
       "  99225,\n",
       "  26381,\n",
       "  99998,\n",
       "  109830,\n",
       "  100631,\n",
       "  99222,\n",
       "  99285,\n",
       "  1773,\n",
       "  101912,\n",
       "  3837,\n",
       "  45181,\n",
       "  104064,\n",
       "  101474,\n",
       "  68862,\n",
       "  104813,\n",
       "  114357,\n",
       "  15946,\n",
       "  87256,\n",
       "  18158,\n",
       "  104954,\n",
       "  3837,\n",
       "  101912,\n",
       "  16,\n",
       "  15,\n",
       "  117033,\n",
       "  3837,\n",
       "  107974,\n",
       "  104887,\n",
       "  32757,\n",
       "  100822,\n",
       "  3837,\n",
       "  101912,\n",
       "  16,\n",
       "  15,\n",
       "  15,\n",
       "  117033,\n",
       "  3837,\n",
       "  103989,\n",
       "  101474,\n",
       "  68862,\n",
       "  1773,\n",
       "  99654,\n",
       "  105177,\n",
       "  101474,\n",
       "  68862,\n",
       "  73670,\n",
       "  101051,\n",
       "  106873,\n",
       "  108010,\n",
       "  3407,\n",
       "  101889,\n",
       "  3837,\n",
       "  115391,\n",
       "  99544,\n",
       "  99225,\n",
       "  26381,\n",
       "  103920,\n",
       "  3837,\n",
       "  99804,\n",
       "  45861,\n",
       "  105340,\n",
       "  106760,\n",
       "  1773,\n",
       "  102029,\n",
       "  99918,\n",
       "  100043,\n",
       "  100316,\n",
       "  30858,\n",
       "  107819,\n",
       "  104460,\n",
       "  99804,\n",
       "  45861,\n",
       "  99730,\n",
       "  32876,\n",
       "  108067,\n",
       "  100631,\n",
       "  99471,\n",
       "  99548,\n",
       "  3837,\n",
       "  101912,\n",
       "  86119,\n",
       "  101047,\n",
       "  102349,\n",
       "  104496,\n",
       "  100146,\n",
       "  17,\n",
       "  24,\n",
       "  15,\n",
       "  111109,\n",
       "  3837,\n",
       "  43288,\n",
       "  87267,\n",
       "  101919,\n",
       "  99471,\n",
       "  99548,\n",
       "  105606,\n",
       "  1773,\n",
       "  85106,\n",
       "  81167,\n",
       "  99487,\n",
       "  99804,\n",
       "  45861,\n",
       "  64471,\n",
       "  88991,\n",
       "  3837,\n",
       "  101034,\n",
       "  104460,\n",
       "  110589,\n",
       "  64471,\n",
       "  102188,\n",
       "  1773,\n",
       "  101912,\n",
       "  3837,\n",
       "  104460,\n",
       "  110589,\n",
       "  16,\n",
       "  18,\n",
       "  22,\n",
       "  64471,\n",
       "  103124,\n",
       "  34,\n",
       "  16,\n",
       "  39,\n",
       "  16,\n",
       "  8996,\n",
       "  18,\n",
       "  13935,\n",
       "  39,\n",
       "  5066,\n",
       "  9370,\n",
       "  100768,\n",
       "  3837,\n",
       "  87267,\n",
       "  85106,\n",
       "  100345,\n",
       "  102388,\n",
       "  32757,\n",
       "  33108,\n",
       "  108010,\n",
       "  36407,\n",
       "  100768,\n",
       "  3837,\n",
       "  77288,\n",
       "  20002,\n",
       "  87267,\n",
       "  104689,\n",
       "  100700,\n",
       "  100768,\n",
       "  3837,\n",
       "  107525,\n",
       "  105652,\n",
       "  3407,\n",
       "  101948,\n",
       "  3837,\n",
       "  105652,\n",
       "  101047,\n",
       "  108740,\n",
       "  32757,\n",
       "  18158,\n",
       "  107916,\n",
       "  3837,\n",
       "  101912,\n",
       "  37029,\n",
       "  59534,\n",
       "  100058,\n",
       "  35551,\n",
       "  100631,\n",
       "  100756,\n",
       "  41299,\n",
       "  90885,\n",
       "  31548,\n",
       "  3837,\n",
       "  103944,\n",
       "  32757,\n",
       "  18158,\n",
       "  9370,\n",
       "  111076,\n",
       "  1773,\n",
       "  100307,\n",
       "  102912,\n",
       "  100000,\n",
       "  99936,\n",
       "  105652,\n",
       "  3837,\n",
       "  103944,\n",
       "  114357,\n",
       "  106674,\n",
       "  3837,\n",
       "  101153,\n",
       "  106304,\n",
       "  108010,\n",
       "  16530,\n",
       "  99472,\n",
       "  100673,\n",
       "  115956,\n",
       "  3407,\n",
       "  87267,\n",
       "  106750,\n",
       "  101118,\n",
       "  107762,\n",
       "  103201,\n",
       "  3837,\n",
       "  101912,\n",
       "  103135,\n",
       "  107456,\n",
       "  111253,\n",
       "  3837,\n",
       "  71817,\n",
       "  107948,\n",
       "  40090,\n",
       "  3837,\n",
       "  23031,\n",
       "  109659,\n",
       "  102193,\n",
       "  99544,\n",
       "  99225,\n",
       "  26381,\n",
       "  1773,\n",
       "  100632,\n",
       "  102349,\n",
       "  15946,\n",
       "  80443,\n",
       "  104496,\n",
       "  3837,\n",
       "  87267,\n",
       "  18493,\n",
       "  100142,\n",
       "  39907,\n",
       "  15946,\n",
       "  99461,\n",
       "  102298,\n",
       "  3837,\n",
       "  100631,\n",
       "  105652,\n",
       "  110487,\n",
       "  34187,\n",
       "  1773,\n",
       "  77288,\n",
       "  100622,\n",
       "  100700,\n",
       "  105652,\n",
       "  3837,\n",
       "  87267,\n",
       "  85106,\n",
       "  104361,\n",
       "  105064,\n",
       "  3407,\n",
       "  100161,\n",
       "  3837,\n",
       "  100768,\n",
       "  104982,\n",
       "  103920,\n",
       "  3837,\n",
       "  100345,\n",
       "  104460,\n",
       "  110589,\n",
       "  33108,\n",
       "  99544,\n",
       "  99225,\n",
       "  26381,\n",
       "  3837,\n",
       "  99892,\n",
       "  100106,\n",
       "  102044,\n",
       "  12,\n",
       "  56006,\n",
       "  99079,\n",
       "  117250,\n",
       "  3837,\n",
       "  32,\n",
       "  28,\n",
       "  30143,\n",
       "  564,\n",
       "  3837,\n",
       "  90919,\n",
       "  30143,\n",
       "  20412,\n",
       "  100487,\n",
       "  99079,\n",
       "  99544,\n",
       "  99225,\n",
       "  110589,\n",
       "  3837,\n",
       "  77288,\n",
       "  99817,\n",
       "  87267,\n",
       "  11622,\n",
       "  100146,\n",
       "  101630,\n",
       "  104460,\n",
       "  110589,\n",
       "  9909,\n",
       "  16,\n",
       "  4,\n",
       "  220,\n",
       "  16,\n",
       "  6226,\n",
       "  48272,\n",
       "  99999,\n",
       "  85106,\n",
       "  81167,\n",
       "  75317,\n",
       "  64471,\n",
       "  88991,\n",
       "  1773,\n",
       "  77557,\n",
       "  3837,\n",
       "  104460,\n",
       "  110589,\n",
       "  16,\n",
       "  18,\n",
       "  22,\n",
       "  87267,\n",
       "  104442,\n",
       "  18493,\n",
       "  16,\n",
       "  4,\n",
       "  108010,\n",
       "  9909,\n",
       "  16,\n",
       "  70,\n",
       "  14,\n",
       "  16,\n",
       "  15,\n",
       "  15,\n",
       "  1014,\n",
       "  7552,\n",
       "  16872,\n",
       "  3837,\n",
       "  16,\n",
       "  6226,\n",
       "  99225,\n",
       "  38507,\n",
       "  9370,\n",
       "  99544,\n",
       "  99225,\n",
       "  26381,\n",
       "  3837,\n",
       "  99654,\n",
       "  100768,\n",
       "  13343,\n",
       "  85106,\n",
       "  44063,\n",
       "  108010,\n",
       "  105359,\n",
       "  17714,\n",
       "  105004,\n",
       "  75317,\n",
       "  3407,\n",
       "  87267,\n",
       "  20002,\n",
       "  20412,\n",
       "  103998,\n",
       "  100631,\n",
       "  99471,\n",
       "  100067,\n",
       "  99235,\n",
       "  3837,\n",
       "  85106,\n",
       "  102188,\n",
       "  9370,\n",
       "  105652,\n",
       "  36407,\n",
       "  75117,\n",
       "  57191,\n",
       "  101128,\n",
       "  101978,\n",
       "  102054,\n",
       "  3837,\n",
       "  99999,\n",
       "  105652,\n",
       "  30534,\n",
       "  104542,\n",
       "  3837,\n",
       "  101137,\n",
       "  99471,\n",
       "  99548,\n",
       "  100142,\n",
       "  1773,\n",
       "  85106,\n",
       "  103944,\n",
       "  105652,\n",
       "  101047,\n",
       "  103991,\n",
       "  40090,\n",
       "  101103,\n",
       "  104282,\n",
       "  3837,\n",
       "  101912,\n",
       "  32757,\n",
       "  18158,\n",
       "  9370,\n",
       "  110238,\n",
       "  5373,\n",
       "  101474,\n",
       "  68862,\n",
       "  97306,\n",
       "  8863,\n",
       "  5373,\n",
       "  99804,\n",
       "  45861,\n",
       "  50404,\n",
       "  49567,\n",
       "  3837,\n",
       "  71268,\n",
       "  101137,\n",
       "  99471,\n",
       "  99548,\n",
       "  101914,\n",
       "  104339,\n",
       "  3837,\n",
       "  99654,\n",
       "  59151,\n",
       "  99306,\n",
       "  101650,\n",
       "  3407,\n",
       "  102050,\n",
       "  100194,\n",
       "  3837,\n",
       "  105652,\n",
       "  104583,\n",
       "  5122,\n",
       "  18158,\n",
       "  90885,\n",
       "  5373,\n",
       "  105177,\n",
       "  101474,\n",
       "  68862,\n",
       "  5373,\n",
       "  115391,\n",
       "  99544,\n",
       "  99225,\n",
       "  26381,\n",
       "  5373,\n",
       "  100768,\n",
       "  104982,\n",
       "  1773,\n",
       "  91572,\n",
       "  30534,\n",
       "  104046,\n",
       "  39907,\n",
       "  9370,\n",
       "  102188,\n",
       "  105178,\n",
       "  99471,\n",
       "  99548,\n",
       "  101914,\n",
       "  3837,\n",
       "  115987,\n",
       "  99392,\n",
       "  100346,\n",
       "  100142,\n",
       "  39907,\n",
       "  3837,\n",
       "  113546,\n",
       "  26381,\n",
       "  44636,\n",
       "  8997,\n",
       "  151668,\n",
       "  715,\n",
       "  6567,\n",
       "  60757,\n",
       "  52801,\n",
       "  3837,\n",
       "  115391,\n",
       "  102029,\n",
       "  99918,\n",
       "  100043,\n",
       "  100316,\n",
       "  30858,\n",
       "  107833,\n",
       "  100058,\n",
       "  9370,\n",
       "  104982,\n",
       "  102119,\n",
       "  101910,\n",
       "  114794,\n",
       "  12,\n",
       "  101479,\n",
       "  17177,\n",
       "  99225,\n",
       "  99225,\n",
       "  26381,\n",
       "  24339,\n",
       "  1773,\n",
       "  101140,\n",
       "  3837,\n",
       "  85106,\n",
       "  108740,\n",
       "  32757,\n",
       "  18158,\n",
       "  106968,\n",
       "  9370,\n",
       "  107833,\n",
       "  100058,\n",
       "  9909,\n",
       "  104995,\n",
       "  106153,\n",
       "  16,\n",
       "  15,\n",
       "  15,\n",
       "  118930,\n",
       "  102029,\n",
       "  99918,\n",
       "  100043,\n",
       "  100316,\n",
       "  30858,\n",
       "  48272,\n",
       "  105278,\n",
       "  113569,\n",
       "  17,\n",
       "  20,\n",
       "  15,\n",
       "  117033,\n",
       "  9370,\n",
       "  32757,\n",
       "  100822,\n",
       "  15946,\n",
       "  90395,\n",
       "  103135,\n",
       "  101474,\n",
       "  68862,\n",
       "  56137,\n",
       "  99483,\n",
       "  26381,\n",
       "  43268,\n",
       "  3837,\n",
       "  100418,\n",
       "  100307,\n",
       "  102912,\n",
       "  1773,\n",
       "  101889,\n",
       "  3837,\n",
       "  45181,\n",
       "  99487,\n",
       "  114357,\n",
       "  15946,\n",
       "  108740,\n",
       "  32757,\n",
       "  18158,\n",
       "  16,\n",
       "  15,\n",
       "  117033,\n",
       "  3837,\n",
       "  105851,\n",
       "  16,\n",
       "  15,\n",
       "  15,\n",
       "  117033,\n",
       "  9370,\n",
       "  32757,\n",
       "  100822,\n",
       "  15946,\n",
       "  3837,\n",
       "  103989,\n",
       "  103135,\n",
       "  101474,\n",
       "  68862,\n",
       "  56137,\n",
       "  99483,\n",
       "  26381,\n",
       "  43268,\n",
       "  3837,\n",
       "  100307,\n",
       "  102912,\n",
       "  1773,\n",
       "  100161,\n",
       "  3837,\n",
       "  37029,\n",
       "  114794,\n",
       "  12,\n",
       "  101479,\n",
       "  17177,\n",
       "  99225,\n",
       "  99225,\n",
       "  26381,\n",
       "  37643,\n",
       "  96050,\n",
       "  17,\n",
       "  24,\n",
       "  15,\n",
       "  111109,\n",
       "  9370,\n",
       "  99804,\n",
       "  45861,\n",
       "  44290,\n",
       "  115391,\n",
       "  99544,\n",
       "  99225,\n",
       "  26381,\n",
       "  3837,\n",
       "  100345,\n",
       "  34,\n",
       "  16,\n",
       "  39,\n",
       "  16,\n",
       "  8996,\n",
       "  18,\n",
       "  13935,\n",
       "  39,\n",
       "  5066,\n",
       "  9370,\n",
       "  104460,\n",
       "  110589,\n",
       "  16,\n",
       "  18,\n",
       "  22,\n",
       "  36407,\n",
       "  100768,\n",
       "  102029,\n",
       "  99918,\n",
       "  100043,\n",
       "  100316,\n",
       "  30858,\n",
       "  9370,\n",
       "  104982,\n",
       "  1773,\n",
       "  115195,\n",
       "  102188,\n",
       "  101650,\n",
       "  3837,\n",
       "  20412,\n",
       "  99471,\n",
       "  99548,\n",
       "  101914,\n",
       "  9370,\n",
       "  115391,\n",
       "  39907,\n",
       "  1773,\n",
       "  151643],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " 'labels': [-100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  151667,\n",
       "  106287,\n",
       "  3837,\n",
       "  20002,\n",
       "  56007,\n",
       "  100146,\n",
       "  100007,\n",
       "  115391,\n",
       "  102029,\n",
       "  99918,\n",
       "  100043,\n",
       "  100316,\n",
       "  30858,\n",
       "  107833,\n",
       "  100058,\n",
       "  9370,\n",
       "  104982,\n",
       "  3837,\n",
       "  100398,\n",
       "  105652,\n",
       "  102021,\n",
       "  1773,\n",
       "  101140,\n",
       "  3837,\n",
       "  35946,\n",
       "  85106,\n",
       "  104843,\n",
       "  100158,\n",
       "  105470,\n",
       "  101042,\n",
       "  39907,\n",
       "  3837,\n",
       "  104050,\n",
       "  99471,\n",
       "  99548,\n",
       "  15946,\n",
       "  103229,\n",
       "  104339,\n",
       "  1773,\n",
       "  102029,\n",
       "  99918,\n",
       "  100043,\n",
       "  100316,\n",
       "  30858,\n",
       "  100409,\n",
       "  104459,\n",
       "  101042,\n",
       "  101047,\n",
       "  101536,\n",
       "  105024,\n",
       "  3837,\n",
       "  102119,\n",
       "  87267,\n",
       "  11622,\n",
       "  114794,\n",
       "  17177,\n",
       "  99225,\n",
       "  99225,\n",
       "  26381,\n",
       "  24339,\n",
       "  100631,\n",
       "  39,\n",
       "  2916,\n",
       "  34,\n",
       "  106896,\n",
       "  3407,\n",
       "  101140,\n",
       "  3837,\n",
       "  20002,\n",
       "  104496,\n",
       "  100146,\n",
       "  114794,\n",
       "  12,\n",
       "  101479,\n",
       "  17177,\n",
       "  99225,\n",
       "  99225,\n",
       "  26381,\n",
       "  24339,\n",
       "  3837,\n",
       "  43288,\n",
       "  104583,\n",
       "  99792,\n",
       "  101041,\n",
       "  104339,\n",
       "  1773,\n",
       "  99212,\n",
       "  105652,\n",
       "  100363,\n",
       "  3837,\n",
       "  101140,\n",
       "  99730,\n",
       "  101118,\n",
       "  111253,\n",
       "  9370,\n",
       "  43316,\n",
       "  56278,\n",
       "  1773,\n",
       "  99519,\n",
       "  107833,\n",
       "  100058,\n",
       "  108010,\n",
       "  87267,\n",
       "  105540,\n",
       "  3837,\n",
       "  85106,\n",
       "  101474,\n",
       "  68862,\n",
       "  26939,\n",
       "  106873,\n",
       "  108010,\n",
       "  101121,\n",
       "  3837,\n",
       "  104193,\n",
       "  99544,\n",
       "  99225,\n",
       "  26381,\n",
       "  18493,\n",
       "  106550,\n",
       "  9370,\n",
       "  43268,\n",
       "  33071,\n",
       "  104589,\n",
       "  1773,\n",
       "  101912,\n",
       "  3837,\n",
       "  60726,\n",
       "  18158,\n",
       "  99623,\n",
       "  32757,\n",
       "  9370,\n",
       "  107833,\n",
       "  100058,\n",
       "  3837,\n",
       "  101889,\n",
       "  101474,\n",
       "  68862,\n",
       "  26939,\n",
       "  106168,\n",
       "  110238,\n",
       "  3837,\n",
       "  101912,\n",
       "  17,\n",
       "  20,\n",
       "  15,\n",
       "  117033,\n",
       "  9370,\n",
       "  32757,\n",
       "  100822,\n",
       "  3837,\n",
       "  99654,\n",
       "  87267,\n",
       "  101909,\n",
       "  105652,\n",
       "  3407,\n",
       "  104326,\n",
       "  3837,\n",
       "  87267,\n",
       "  85106,\n",
       "  100642,\n",
       "  101474,\n",
       "  68862,\n",
       "  3837,\n",
       "  99519,\n",
       "  104064,\n",
       "  101474,\n",
       "  68862,\n",
       "  104813,\n",
       "  114357,\n",
       "  87267,\n",
       "  99544,\n",
       "  99225,\n",
       "  26381,\n",
       "  99998,\n",
       "  109830,\n",
       "  100631,\n",
       "  99222,\n",
       "  99285,\n",
       "  1773,\n",
       "  101912,\n",
       "  3837,\n",
       "  45181,\n",
       "  104064,\n",
       "  101474,\n",
       "  68862,\n",
       "  104813,\n",
       "  114357,\n",
       "  15946,\n",
       "  87256,\n",
       "  18158,\n",
       "  104954,\n",
       "  3837,\n",
       "  101912,\n",
       "  16,\n",
       "  15,\n",
       "  117033,\n",
       "  3837,\n",
       "  107974,\n",
       "  104887,\n",
       "  32757,\n",
       "  100822,\n",
       "  3837,\n",
       "  101912,\n",
       "  16,\n",
       "  15,\n",
       "  15,\n",
       "  117033,\n",
       "  3837,\n",
       "  103989,\n",
       "  101474,\n",
       "  68862,\n",
       "  1773,\n",
       "  99654,\n",
       "  105177,\n",
       "  101474,\n",
       "  68862,\n",
       "  73670,\n",
       "  101051,\n",
       "  106873,\n",
       "  108010,\n",
       "  3407,\n",
       "  101889,\n",
       "  3837,\n",
       "  115391,\n",
       "  99544,\n",
       "  99225,\n",
       "  26381,\n",
       "  103920,\n",
       "  3837,\n",
       "  99804,\n",
       "  45861,\n",
       "  105340,\n",
       "  106760,\n",
       "  1773,\n",
       "  102029,\n",
       "  99918,\n",
       "  100043,\n",
       "  100316,\n",
       "  30858,\n",
       "  107819,\n",
       "  104460,\n",
       "  99804,\n",
       "  45861,\n",
       "  99730,\n",
       "  32876,\n",
       "  108067,\n",
       "  100631,\n",
       "  99471,\n",
       "  99548,\n",
       "  3837,\n",
       "  101912,\n",
       "  86119,\n",
       "  101047,\n",
       "  102349,\n",
       "  104496,\n",
       "  100146,\n",
       "  17,\n",
       "  24,\n",
       "  15,\n",
       "  111109,\n",
       "  3837,\n",
       "  43288,\n",
       "  87267,\n",
       "  101919,\n",
       "  99471,\n",
       "  99548,\n",
       "  105606,\n",
       "  1773,\n",
       "  85106,\n",
       "  81167,\n",
       "  99487,\n",
       "  99804,\n",
       "  45861,\n",
       "  64471,\n",
       "  88991,\n",
       "  3837,\n",
       "  101034,\n",
       "  104460,\n",
       "  110589,\n",
       "  64471,\n",
       "  102188,\n",
       "  1773,\n",
       "  101912,\n",
       "  3837,\n",
       "  104460,\n",
       "  110589,\n",
       "  16,\n",
       "  18,\n",
       "  22,\n",
       "  64471,\n",
       "  103124,\n",
       "  34,\n",
       "  16,\n",
       "  39,\n",
       "  16,\n",
       "  8996,\n",
       "  18,\n",
       "  13935,\n",
       "  39,\n",
       "  5066,\n",
       "  9370,\n",
       "  100768,\n",
       "  3837,\n",
       "  87267,\n",
       "  85106,\n",
       "  100345,\n",
       "  102388,\n",
       "  32757,\n",
       "  33108,\n",
       "  108010,\n",
       "  36407,\n",
       "  100768,\n",
       "  3837,\n",
       "  77288,\n",
       "  20002,\n",
       "  87267,\n",
       "  104689,\n",
       "  100700,\n",
       "  100768,\n",
       "  3837,\n",
       "  107525,\n",
       "  105652,\n",
       "  3407,\n",
       "  101948,\n",
       "  3837,\n",
       "  105652,\n",
       "  101047,\n",
       "  108740,\n",
       "  32757,\n",
       "  18158,\n",
       "  107916,\n",
       "  3837,\n",
       "  101912,\n",
       "  37029,\n",
       "  59534,\n",
       "  100058,\n",
       "  35551,\n",
       "  100631,\n",
       "  100756,\n",
       "  41299,\n",
       "  90885,\n",
       "  31548,\n",
       "  3837,\n",
       "  103944,\n",
       "  32757,\n",
       "  18158,\n",
       "  9370,\n",
       "  111076,\n",
       "  1773,\n",
       "  100307,\n",
       "  102912,\n",
       "  100000,\n",
       "  99936,\n",
       "  105652,\n",
       "  3837,\n",
       "  103944,\n",
       "  114357,\n",
       "  106674,\n",
       "  3837,\n",
       "  101153,\n",
       "  106304,\n",
       "  108010,\n",
       "  16530,\n",
       "  99472,\n",
       "  100673,\n",
       "  115956,\n",
       "  3407,\n",
       "  87267,\n",
       "  106750,\n",
       "  101118,\n",
       "  107762,\n",
       "  103201,\n",
       "  3837,\n",
       "  101912,\n",
       "  103135,\n",
       "  107456,\n",
       "  111253,\n",
       "  3837,\n",
       "  71817,\n",
       "  107948,\n",
       "  40090,\n",
       "  3837,\n",
       "  23031,\n",
       "  109659,\n",
       "  102193,\n",
       "  99544,\n",
       "  99225,\n",
       "  26381,\n",
       "  1773,\n",
       "  100632,\n",
       "  102349,\n",
       "  15946,\n",
       "  80443,\n",
       "  104496,\n",
       "  3837,\n",
       "  87267,\n",
       "  18493,\n",
       "  100142,\n",
       "  39907,\n",
       "  15946,\n",
       "  99461,\n",
       "  102298,\n",
       "  3837,\n",
       "  100631,\n",
       "  105652,\n",
       "  110487,\n",
       "  34187,\n",
       "  1773,\n",
       "  77288,\n",
       "  100622,\n",
       "  100700,\n",
       "  105652,\n",
       "  3837,\n",
       "  87267,\n",
       "  85106,\n",
       "  104361,\n",
       "  105064,\n",
       "  3407,\n",
       "  100161,\n",
       "  3837,\n",
       "  100768,\n",
       "  104982,\n",
       "  103920,\n",
       "  3837,\n",
       "  100345,\n",
       "  104460,\n",
       "  110589,\n",
       "  33108,\n",
       "  99544,\n",
       "  99225,\n",
       "  26381,\n",
       "  3837,\n",
       "  99892,\n",
       "  100106,\n",
       "  102044,\n",
       "  12,\n",
       "  56006,\n",
       "  99079,\n",
       "  117250,\n",
       "  3837,\n",
       "  32,\n",
       "  28,\n",
       "  30143,\n",
       "  564,\n",
       "  3837,\n",
       "  90919,\n",
       "  30143,\n",
       "  20412,\n",
       "  100487,\n",
       "  99079,\n",
       "  99544,\n",
       "  99225,\n",
       "  110589,\n",
       "  3837,\n",
       "  77288,\n",
       "  99817,\n",
       "  87267,\n",
       "  11622,\n",
       "  100146,\n",
       "  101630,\n",
       "  104460,\n",
       "  110589,\n",
       "  9909,\n",
       "  16,\n",
       "  4,\n",
       "  220,\n",
       "  16,\n",
       "  6226,\n",
       "  48272,\n",
       "  99999,\n",
       "  85106,\n",
       "  81167,\n",
       "  75317,\n",
       "  64471,\n",
       "  88991,\n",
       "  1773,\n",
       "  77557,\n",
       "  3837,\n",
       "  104460,\n",
       "  110589,\n",
       "  16,\n",
       "  18,\n",
       "  22,\n",
       "  87267,\n",
       "  104442,\n",
       "  18493,\n",
       "  16,\n",
       "  4,\n",
       "  108010,\n",
       "  9909,\n",
       "  16,\n",
       "  70,\n",
       "  14,\n",
       "  16,\n",
       "  15,\n",
       "  15,\n",
       "  1014,\n",
       "  7552,\n",
       "  16872,\n",
       "  3837,\n",
       "  16,\n",
       "  6226,\n",
       "  99225,\n",
       "  38507,\n",
       "  9370,\n",
       "  99544,\n",
       "  99225,\n",
       "  26381,\n",
       "  3837,\n",
       "  99654,\n",
       "  100768,\n",
       "  13343,\n",
       "  85106,\n",
       "  44063,\n",
       "  108010,\n",
       "  105359,\n",
       "  17714,\n",
       "  105004,\n",
       "  75317,\n",
       "  3407,\n",
       "  87267,\n",
       "  20002,\n",
       "  20412,\n",
       "  103998,\n",
       "  100631,\n",
       "  99471,\n",
       "  100067,\n",
       "  99235,\n",
       "  3837,\n",
       "  85106,\n",
       "  102188,\n",
       "  9370,\n",
       "  105652,\n",
       "  36407,\n",
       "  75117,\n",
       "  57191,\n",
       "  101128,\n",
       "  101978,\n",
       "  102054,\n",
       "  3837,\n",
       "  99999,\n",
       "  105652,\n",
       "  30534,\n",
       "  104542,\n",
       "  3837,\n",
       "  101137,\n",
       "  99471,\n",
       "  99548,\n",
       "  100142,\n",
       "  1773,\n",
       "  85106,\n",
       "  103944,\n",
       "  105652,\n",
       "  101047,\n",
       "  103991,\n",
       "  40090,\n",
       "  101103,\n",
       "  104282,\n",
       "  3837,\n",
       "  101912,\n",
       "  32757,\n",
       "  18158,\n",
       "  9370,\n",
       "  110238,\n",
       "  5373,\n",
       "  101474,\n",
       "  68862,\n",
       "  97306,\n",
       "  8863,\n",
       "  5373,\n",
       "  99804,\n",
       "  45861,\n",
       "  50404,\n",
       "  49567,\n",
       "  3837,\n",
       "  71268,\n",
       "  101137,\n",
       "  99471,\n",
       "  99548,\n",
       "  101914,\n",
       "  104339,\n",
       "  3837,\n",
       "  99654,\n",
       "  59151,\n",
       "  99306,\n",
       "  101650,\n",
       "  3407,\n",
       "  102050,\n",
       "  100194,\n",
       "  3837,\n",
       "  105652,\n",
       "  104583,\n",
       "  5122,\n",
       "  18158,\n",
       "  90885,\n",
       "  5373,\n",
       "  105177,\n",
       "  101474,\n",
       "  68862,\n",
       "  5373,\n",
       "  115391,\n",
       "  99544,\n",
       "  99225,\n",
       "  26381,\n",
       "  5373,\n",
       "  100768,\n",
       "  104982,\n",
       "  1773,\n",
       "  91572,\n",
       "  30534,\n",
       "  104046,\n",
       "  39907,\n",
       "  9370,\n",
       "  102188,\n",
       "  105178,\n",
       "  99471,\n",
       "  99548,\n",
       "  101914,\n",
       "  3837,\n",
       "  115987,\n",
       "  99392,\n",
       "  100346,\n",
       "  100142,\n",
       "  39907,\n",
       "  3837,\n",
       "  113546,\n",
       "  26381,\n",
       "  44636,\n",
       "  8997,\n",
       "  151668,\n",
       "  715,\n",
       "  6567,\n",
       "  60757,\n",
       "  52801,\n",
       "  3837,\n",
       "  115391,\n",
       "  102029,\n",
       "  99918,\n",
       "  100043,\n",
       "  100316,\n",
       "  30858,\n",
       "  107833,\n",
       "  100058,\n",
       "  9370,\n",
       "  104982,\n",
       "  102119,\n",
       "  101910,\n",
       "  114794,\n",
       "  12,\n",
       "  101479,\n",
       "  17177,\n",
       "  99225,\n",
       "  99225,\n",
       "  26381,\n",
       "  24339,\n",
       "  1773,\n",
       "  101140,\n",
       "  3837,\n",
       "  85106,\n",
       "  108740,\n",
       "  32757,\n",
       "  18158,\n",
       "  106968,\n",
       "  9370,\n",
       "  107833,\n",
       "  100058,\n",
       "  9909,\n",
       "  104995,\n",
       "  106153,\n",
       "  16,\n",
       "  15,\n",
       "  15,\n",
       "  118930,\n",
       "  102029,\n",
       "  99918,\n",
       "  100043,\n",
       "  100316,\n",
       "  30858,\n",
       "  48272,\n",
       "  105278,\n",
       "  113569,\n",
       "  17,\n",
       "  20,\n",
       "  15,\n",
       "  117033,\n",
       "  9370,\n",
       "  32757,\n",
       "  100822,\n",
       "  15946,\n",
       "  90395,\n",
       "  103135,\n",
       "  101474,\n",
       "  68862,\n",
       "  56137,\n",
       "  99483,\n",
       "  26381,\n",
       "  43268,\n",
       "  3837,\n",
       "  100418,\n",
       "  100307,\n",
       "  102912,\n",
       "  1773,\n",
       "  101889,\n",
       "  3837,\n",
       "  45181,\n",
       "  99487,\n",
       "  114357,\n",
       "  15946,\n",
       "  108740,\n",
       "  32757,\n",
       "  18158,\n",
       "  16,\n",
       "  15,\n",
       "  117033,\n",
       "  3837,\n",
       "  105851,\n",
       "  16,\n",
       "  15,\n",
       "  15,\n",
       "  117033,\n",
       "  9370,\n",
       "  32757,\n",
       "  100822,\n",
       "  15946,\n",
       "  3837,\n",
       "  103989,\n",
       "  103135,\n",
       "  101474,\n",
       "  68862,\n",
       "  56137,\n",
       "  99483,\n",
       "  26381,\n",
       "  43268,\n",
       "  3837,\n",
       "  100307,\n",
       "  102912,\n",
       "  1773,\n",
       "  100161,\n",
       "  3837,\n",
       "  37029,\n",
       "  114794,\n",
       "  12,\n",
       "  101479,\n",
       "  17177,\n",
       "  99225,\n",
       "  99225,\n",
       "  26381,\n",
       "  37643,\n",
       "  96050,\n",
       "  17,\n",
       "  24,\n",
       "  15,\n",
       "  111109,\n",
       "  9370,\n",
       "  99804,\n",
       "  45861,\n",
       "  44290,\n",
       "  115391,\n",
       "  99544,\n",
       "  99225,\n",
       "  26381,\n",
       "  3837,\n",
       "  100345,\n",
       "  34,\n",
       "  16,\n",
       "  39,\n",
       "  16,\n",
       "  8996,\n",
       "  18,\n",
       "  13935,\n",
       "  39,\n",
       "  5066,\n",
       "  9370,\n",
       "  104460,\n",
       "  110589,\n",
       "  16,\n",
       "  18,\n",
       "  22,\n",
       "  36407,\n",
       "  100768,\n",
       "  102029,\n",
       "  99918,\n",
       "  100043,\n",
       "  100316,\n",
       "  30858,\n",
       "  9370,\n",
       "  104982,\n",
       "  1773,\n",
       "  115195,\n",
       "  102188,\n",
       "  101650,\n",
       "  3837,\n",
       "  20412,\n",
       "  99471,\n",
       "  99548,\n",
       "  101914,\n",
       "  9370,\n",
       "  115391,\n",
       "  39907,\n",
       "  1773,\n",
       "  151643]}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6d62e5e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "707be2bdb8954977a9ea06e36721f366",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/241 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# 得到验证集\n",
    "eval_df = pd.read_json(test_jsonl_new_path, lines=True)\n",
    "eval_ds = Dataset.from_pandas(eval_df)\n",
    "eval_dataset = eval_ds.map(process_func, remove_columns=eval_ds.column_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d422bfdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "929"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset[2]['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e51f93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "56da4e9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6a0b05e3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Question: 1895年德国物理学教授伦琴的发现对医学影像学的发展有何具体影响？请从技术进步、学科建立和临床应用三个方面进行分析。\n",
      "\n",
      "    LLM:<think>\n",
      "好的，用户问的是1895年伦琴发现X射线对医学影像学的影响，需要从技术进步、学科建立和临床应用三个方面分析。首先，我得回忆一下伦琴的发现背景，X射线的发现确实开创了医学影像学的先河。\n",
      "\n",
      "技术进步方面，X射线的发现直接推动了X光机的发明，这应该是关键点。早期的X光机虽然有局限，但确实让医生能够看到内部结构，比如骨骼和器官。可能还要提到后续技术的发展，比如计算机断层扫描（CT）和磁共振成像（MRI）这些，但用户可能只需要1895年之后的影响，所以可能不需要太深入，但需要指出X射线作为基础技术的重要性。\n",
      "\n",
      "学科建立方面，伦琴的发现促使医学影像学成为独立的学科。之前医学和物理学可能分开，但X射线的应用让医学需要更系统的影像学研究，所以学科的建立应该提到，比如1901年德国的弗里德里希·洛温哈特创立了医学影像学，或者类似的时间点。另外，可能涉及影像学作为一门科学的独立发展，比如研究X射线的性质、成像原理等。\n",
      "\n",
      "临床应用方面，X射线在诊断骨折、肺部疾病、牙齿问题等方面的应用，改变了传统诊断方法。比如，X光片成为常规检查手段，提高了诊断效率和准确性。可能还要提到后续的发展，比如数字化成像和增强现实技术，但用户可能只需要1895年的影响，所以重点在初期应用。\n",
      "\n",
      "需要确认每个方面的具体例子，比如技术进步中的X光机，学科建立中的学科名称和时间，临床应用中的具体疾病诊断例子。同时要注意时间线，1895年是发现，之后1901年学科建立，然后是临床应用的发展。可能还需要提到伦琴的贡献，比如首次成功应用X射线进行医学诊断，比如发现肺部结核或骨折。\n",
      "\n",
      "另外，用户可能希望了解影响的层次，技术、学科、临床各方面的具体影响，所以需要分点详细说明。可能还要注意术语的准确性，比如“医学影像学”是否在1895年就成为独立学科，或者更晚才确立。比如，1901年洛温哈特可能创立了医学影像学，所以学科建立方面需要提到这个时间点。\n",
      "\n",
      "总结起来，结构应该是：技术进步（X光机的发明和应用）、学科建立（医学影像学作为独立学科的形成）、临床应用（诊断方法的革新和疾病检测）。每个部分都要有具体例子和时间点，确保回答全面且准确。\n",
      "</think>\n",
      "\n",
      "1895年威廉·伦琴（Wilhelm Roentgen）发现X射线对医学影像学的发展具有里程碑式的影响，其影响可从**技术进步**、**学科建立**和**临床应用**三个方面具体分析：\n",
      "\n",
      "---\n",
      "\n",
      "### **1. 技术进步：X光机的发明与影像技术的突破**\n",
      "- **基础技术的诞生**：X射线的发现直接推动了医学影像技术的诞生。伦琴首次通过实验发现X射线能穿透软组织并形成影像，为医学提供了一种非侵入性的诊断工具。\n",
      "- **X光机的普及**：1896年，伦琴发明了第一台X光机，使医生能够通过X光片观察人体内部结构（如骨骼、肺部、牙齿等）。这一技术突破解决了传统医学（如尸体解剖）的局限性，为后续影像技术（如CT、MRI）奠定了基础。\n",
      "- **技术延伸**：X射线的发现催生了多种影像设备，如X光胶片、胶片相机、数字成像系统等，逐步推动影像技术从“实验性”向“临床实用化”发展。\n",
      "\n",
      "---\n",
      "\n",
      "### **2. 学科建立：医学影像学的独立学科化**\n",
      "- **医学与物理学的交叉**：X射线的发现标志着医学从单纯的解剖学向影像学、放射学等跨学科领域发展。伦琴的成果促使医学界将影像学作为独立学科，与解剖学、病理学等并列。\n",
      "- **学科体系的形成**：1901年，德国学者弗里德里希·洛温哈特（Friedrich Löffler）提出“医学影像学”（Medizinische Bildung）的概念，明确影像学作为医学分支的学科地位。此后，医学影像学逐渐发展为一门独立的学科，涵盖影像设备、成像原理、诊断标准等。\n",
      "- **研究方法的完善**：X射线的发现推动了对影像学原理（如辐射物理、图像形成机制）的研究，为后续影像技术的优化和标准化提供了理论基础。\n",
      "\n",
      "---\n",
      "\n",
      "### **3. 临床应用：诊断效率与医学实践的革新**\n",
      "- **疾病诊断的革命**：X射线成为诊断骨折、肺部疾病（如结核）、牙病、心脏病等的首选方法。例如，X光片能快速识别骨骼骨折，为外科手术提供重要依据。\n",
      "- **早期癌症筛查**：X射线首次用于检测肺部肿瘤（如肺结核），为癌症早期发现提供了手段，显著提高了治疗成功率。\n",
      "- **临床实践的标准化**：X射线的普及促使医学影像学成为临床常规检查的一部分，推动了影像学检查的标准化流程（如X光片的摄取、分析、报告等）。\n",
      "- **后续技术的奠基**：X射线的发现为CT（计算机断层扫描）、MRI（磁共振成像）等先进影像技术提供了基础，使医学影像学从“单次成像”发展为“多维度、高精度”的诊断工具。\n",
      "\n",
      "---\n",
      "\n",
      "### **总结**\n",
      "伦琴的X射线发现不仅解决了医学诊断的瓶颈，还推动了医学影像学从实验技术向临床学科的转型。技术上奠定了影像诊断的基础，学科上确立了影像学的独立地位，临床应用上彻底改变了医学诊断方式，成为现代医学的重要支柱。这一发现至今仍是医学影像学发展的起点，其影响延续至今，贯穿于医学影像技术的演进与临床实践的革新。\n",
      "    \n",
      "\n",
      "    Question: 医生，我最近胃部不适，听说有几种抗溃疡药物可以治疗，您能详细介绍一下这些药物的分类、作用机制以及它们是如何影响胃黏膜的保护与损伤平衡的吗？\n",
      "\n",
      "    LLM:<think>\n",
      "好的，用户问的是关于抗溃疡药物的分类、作用机制以及它们如何影响胃黏膜的保护与损伤平衡。首先，我需要回忆一下常见的抗溃疡药物，比如质子泵抑制剂（PPIs）、H2受体拮抗剂、胃黏膜保护剂和促胃动力药。\n",
      "\n",
      "用户可能对这些药物不太了解，所以需要详细解释每个类别的作用机制。比如PPIs通过抑制胃壁细胞的H+分泌来减少胃酸，从而保护胃黏膜。H2受体拮抗剂则通过阻断胃壁的H2受体来减少酸分泌，但效果可能不如PPIs强。胃黏膜保护剂如铝碳酸镁可以形成保护膜，而促胃动力药如多潘立酮可能通过促进胃排空来减少胃酸分泌。\n",
      "\n",
      "接下来要考虑用户可能的深层需求。他们可能想知道哪种药物更适合他们，或者如何选择药物。因此，在回答时需要提到不同药物的适用情况，比如PPIs适用于顽固性溃疡，而H2受体拮抗剂可能用于轻度症状。同时，要强调药物的副作用，比如PPIs长期使用可能有耐药性或骨质疏松风险。\n",
      "\n",
      "还需要注意用户可能没有提到的点，比如药物的相互作用、用药时间、剂量调整等。比如，PPIs通常需要长期使用，而H2受体拮抗剂可能需要更短疗程。此外，胃黏膜保护剂可能需要配合PPIs使用，以增强保护效果。\n",
      "\n",
      "最后，确保回答结构清晰，分点说明药物分类、作用机制、影响胃黏膜的平衡，以及注意事项。这样用户能一目了然地理解不同药物的作用和使用方法。\n",
      "</think>\n",
      "\n",
      "胃部不适的治疗需要根据溃疡类型（如胃溃疡、十二指肠溃疡）和病因（如幽门螺杆菌感染）选择合适的药物。以下是抗溃疡药物的分类、作用机制及对胃黏膜保护与损伤平衡的影响：\n",
      "\n",
      "---\n",
      "\n",
      "### **一、抗溃疡药物的分类**\n",
      "1. **质子泵抑制剂（PPIs）**  \n",
      "   - **作用机制**：通过抑制胃壁细胞中的H+-K+ ATP酶（质子泵），阻断胃酸分泌。  \n",
      "   - **典型药物**：奥美拉唑（Omeprazole）、兰索拉唑（Lansoprazole）等。  \n",
      "   - **对胃黏膜的影响**：  \n",
      "     - **保护作用**：减少胃酸对黏膜的直接损伤，促进黏膜修复。  \n",
      "     - **损伤平衡**：长期使用可能破坏胃黏膜屏障，导致耐药性或骨质疏松（需注意用药时间）。  \n",
      "\n",
      "2. **H2受体拮抗剂**  \n",
      "   - **作用机制**：阻断胃壁胃酸分泌中的H2受体，减少胃酸分泌。  \n",
      "   - **典型药物**：雷尼替丁（Ranitidine）、法莫替丁（Famotidine）。  \n",
      "   - **对胃黏膜的影响**：  \n",
      "     - **保护作用**：效果较PPIs弱，但适合短期或轻度溃疡。  \n",
      "     - **损伤平衡**：长期使用可能降低胃酸防御能力，需与PPIs联合使用（如幽门螺杆菌治疗）。  \n",
      "\n",
      "3. **胃黏膜保护剂**  \n",
      "   - **作用机制**：通过形成保护性黏膜屏障或促进黏膜修复。  \n",
      "   - **典型药物**：铝碳酸镁（Almagel）、硫糖铝（Sucralfate）。  \n",
      "   - **对胃黏膜的影响**：  \n",
      "     - **保护作用**：直接覆盖溃疡面，减少胃酸和消化酶的侵蚀。  \n",
      "     - **损伤平衡**：需配合PPIs或H2受体拮抗剂以增强效果，长期使用可能引起便秘或腹泻。  \n",
      "\n",
      "4. **促胃动力药**  \n",
      "   - **作用机制**：通过加速胃排空，减少胃酸反流。  \n",
      "   - **典型药物**：多潘立酮（Domperidone）、莫沙必利（Mosapride）。  \n",
      "   - **对胃黏膜的影响**：  \n",
      "     - **保护作用**：减少胃酸反流对黏膜的刺激。  \n",
      "     - **损伤平衡**：需与其他药物联合使用（如PPIs），避免因胃排空过快导致溃疡加重。  \n",
      "\n",
      "---\n",
      "\n",
      "### **二、药物对胃黏膜保护与损伤平衡的影响**\n",
      "1. **酸分泌抑制**  \n",
      "   - PPIs和H2受体拮抗剂通过抑制胃酸分泌，直接减少对黏膜的损伤，但长期使用可能破坏黏膜屏障（如PPIs的耐药性）。  \n",
      "   - **关键点**：需在医生指导下使用，避免长期依赖。\n",
      "\n",
      "2. **黏膜修复与再生**  \n",
      "   - PPIs通过降低胃酸，促进胃黏膜上皮细胞修复，加速溃疡愈合。  \n",
      "   - 胶体铋剂（如硫糖铝）可形成保护膜，促进黏膜再生。\n",
      "\n",
      "3. **黏膜屏障功能**  \n",
      "   - 长期使用PPIs可能削弱胃黏膜的自我保护能力（如黏膜血流减少），需定期监测（如骨密度、胃酸分泌功能）。\n",
      "\n",
      "4. **副作用与耐药性**  \n",
      "   - **耐药性**：PPIs长期使用可能使胃酸分泌能力下降，需更换药物或联合其他药物。  \n",
      "   - **副作用**：PPIs可能引起腹泻、骨折风险；H2受体拮抗剂可能引起胃部不适；胃黏膜保护剂可能引起便秘。\n",
      "\n",
      "---\n",
      "\n",
      "### **三、临床应用建议**\n",
      "1. **幽门螺杆菌感染**：  \n",
      "   - 通常采用PPIs + 铋剂（如硫糖铝） + 抗生素（如阿莫西林）的三联疗法，疗程10-14天。  \n",
      "   - 需严格按医嘱用药，避免自行停药。\n",
      "\n",
      "2. **慢性胃溃疡/十二指肠溃疡**：  \n",
      "   - PPIs是首选药物，需联合胃黏膜保护剂（如硫糖铝）以增强疗效。\n",
      "\n",
      "3. **急性胃炎或反流性食管炎**：  \n",
      "   - 促胃动力药（如多潘立酮）可缓解反流，但需避免长期使用。\n",
      "\n",
      "4. **特殊人群**：  \n",
      "   - 老年人或骨质疏松患者需谨慎使用PPIs；儿童需避免使用H2受体拮抗剂。\n",
      "\n",
      "---\n",
      "\n",
      "### **四、注意事项**\n",
      "- **药物相互作用**：PPIs与某些药物（如NSAIDs）可能增加胃溃疡风险，需避免合用。  \n",
      "- **用药时间**：PPIs通常在餐前30分钟服用，以增强效果。  \n",
      "- **疗程**：幽门螺杆菌治疗需严格按疗程，避免因漏服导致疗效不足。\n",
      "\n",
      "---\n",
      "\n",
      "### **总结**\n",
      "抗溃疡药物通过抑制胃酸分泌、保护黏膜屏障或促进修复，平衡胃黏膜的损伤与修复。选择药物需结合溃疡类型、病因及患者个体差异，长期使用需密切监测副作用。建议在医生指导下个体化治疗，避免自行用药。\n",
      "    \n",
      "\n",
      "    Question: 我最近被诊断出患有淋巴瘤，医生提到这可能导致发热。请问这是由于淋巴瘤组织的坏死和细胞破坏引起的吗？如果是，具体机制是什么？\n",
      "\n",
      "    LLM:<think>\n",
      "好的，用户最近被诊断出淋巴瘤，医生提到可能有发热，想知道是否是由于淋巴瘤组织的坏死和细胞破坏引起的。首先，我需要确认淋巴瘤的发热机制。\n",
      "\n",
      "淋巴瘤是恶性淋巴细胞增殖的疾病，通常分为B细胞和T细胞类型。发热可能由多种因素引起，比如感染、炎症或肿瘤本身的代谢产物。用户提到的坏死和细胞破坏，可能涉及肿瘤组织的坏死，释放炎症因子，导致发热。\n",
      "\n",
      "需要考虑淋巴瘤的类型，比如侵袭性淋巴瘤可能有较高的坏死率，而某些亚型如伯基特淋巴瘤可能有更高的发热率。另外，肿瘤的生长可能引起局部炎症，或者通过释放细胞因子如IL-6、TNF-α导致发热。\n",
      "\n",
      "还要考虑其他原因，比如感染，比如结核或病毒感染，也可能导致发热。但用户的问题集中在肿瘤本身的因素，所以需要重点解释肿瘤坏死和细胞破坏的作用。\n",
      "\n",
      "可能的机制包括：肿瘤细胞的坏死释放炎症介质，引起全身性炎症反应；肿瘤组织的破坏导致白细胞损伤，影响免疫功能，可能引发发热；或者肿瘤本身代谢产物如乳酸堆积，导致代谢紊乱，引发发热。\n",
      "\n",
      "需要确保回答准确，同时区分不同类型的淋巴瘤，以及可能的并发症。还要提醒用户及时就医，进行详细检查，排除其他原因，比如感染或肿瘤进展。\n",
      "</think>\n",
      "\n",
      "淋巴瘤（恶性淋巴细胞增殖性疾病）引起的发热通常与肿瘤的**组织坏死、细胞破坏**以及**免疫系统异常**密切相关。以下是具体机制和可能的解释：\n",
      "\n",
      "---\n",
      "\n",
      "### **1. 肿瘤组织坏死与炎症因子释放**\n",
      "- **坏死机制**：  \n",
      "  淋巴瘤细胞在生长过程中，若肿瘤负荷较大或存在局部浸润，可能导致肿瘤组织的**坏死**。坏死的细胞会释放多种**炎症因子**（如IL-1、TNF-α、IL-6、IL-8等），这些因子会激活免疫系统，引发**全身性炎症反应**，导致发热。\n",
      "- **细胞破坏**：  \n",
      "  肿瘤细胞的破坏可能释放**细胞外基质**（如胶原蛋白、纤维蛋白）和**代谢产物**（如乳酸、β-羟丁酸），这些物质可能通过血液循环引发炎症反应，进一步导致发热。\n",
      "\n",
      "---\n",
      "\n",
      "### **2. 免疫系统异常与全身性炎症**\n",
      "- **免疫系统紊乱**：  \n",
      "  淋巴瘤本身可能抑制免疫细胞的功能（如T细胞、NK细胞），导致机体对感染的防御能力下降，从而可能继发**感染性发热**（如结核、病毒性感染）。\n",
      "- **肿瘤相关炎症**：  \n",
      "  淋巴瘤细胞的增殖和坏死可能直接引发局部炎症，甚至通过**旁分泌机制**（如释放细胞因子）影响周围组织，导致发热。\n",
      "\n",
      "---\n",
      "\n",
      "### **3. 肿瘤的代谢产物与炎症反应**\n",
      "- **代谢产物积累**：  \n",
      "  肿瘤细胞的异常增殖可能导致**乳酸、β-羟丁酸等代谢产物**的积累，这些物质可能通过血液循环引发炎症反应。\n",
      "- **肿瘤因子释放**：  \n",
      "  淋巴瘤细胞可能释放**肿瘤坏死因子（TNF-α）**等促炎因子，进一步激活免疫系统，导致发热。\n",
      "\n",
      "---\n",
      "\n",
      "### **4. 潜在的肿瘤进展与继发性感染**\n",
      "- **肿瘤进展**：  \n",
      "  淋巴瘤若进展至晚期，可能伴随**感染性发热**（如结核、真菌感染）或**肿瘤相关性发热**。\n",
      "- **免疫抑制**：  \n",
      "  淋巴瘤患者常因免疫系统受损而易发生感染，感染可能引发发热。\n",
      "\n",
      "---\n",
      "\n",
      "### **5. 不同类型淋巴瘤的发热特点**\n",
      "- **B细胞淋巴瘤**（如伯基特淋巴瘤）：  \n",
      "  坏死率较高，发热常与肿瘤坏死直接相关。\n",
      "- **T细胞淋巴瘤**：  \n",
      "  发热可能更多与肿瘤浸润导致的局部炎症或免疫抑制有关。\n",
      "- **其他亚型**：  \n",
      "  如弥漫大B细胞淋巴瘤，发热可能与肿瘤负荷、炎症因子释放或继发感染有关。\n",
      "\n",
      "---\n",
      "\n",
      "### **需要进一步检查的项目**\n",
      "- **炎症标志物**（如CRP、ESR）  \n",
      "- **感染病原体筛查**（如结核、病毒、细菌）  \n",
      "- **肿瘤标志物**（如LDH、β-2微球蛋白）  \n",
      "- **影像学检查**（如CT/MRI）评估肿瘤大小和位置  \n",
      "\n",
      "---\n",
      "\n",
      "### **建议**\n",
      "1. **及时就医**：发热可能是肿瘤进展或感染的信号，需结合临床症状（如体重下降、乏力、淋巴结肿大）综合判断。\n",
      "2. **治疗方向**：  \n",
      "   - **化疗**：针对淋巴瘤的标准化治疗（如R-CHOP方案）。  \n",
      "   - **免疫治疗**：如PD-1抑制剂（适用于某些亚型）。  \n",
      "   - **支持治疗**：控制感染、缓解症状。\n",
      "\n",
      "---\n",
      "\n",
      "### **总结**\n",
      "淋巴瘤引起的发热主要由肿瘤组织的坏死和细胞破坏导致的炎症因子释放、免疫系统紊乱，以及可能的继发感染。具体机制需结合肿瘤类型、病情阶段及个体差异综合分析。建议密切随访，及时调整治疗方案。\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "test_df = pd.read_json(test_jsonl_new_path, lines=True)[:3]\n",
    "\n",
    "test_text_list = []\n",
    "\n",
    "for index, row in test_df.iterrows():\n",
    "    instruction = row['instruction']\n",
    "    input_value = row['input']\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": f\"{instruction}\"},\n",
    "        {\"role\": \"user\", \"content\": f\"{input_value}\"}\n",
    "    ]\n",
    "    \n",
    "\n",
    "    response = predict(messages, model, tokenizer)\n",
    "\n",
    "    response_text = f\"\"\"\n",
    "    Question: {input_value}\n",
    "\n",
    "    LLM:{response}\n",
    "    \"\"\"\n",
    "    \n",
    "#     test_text_list.append(swanlab.Text(response_text))\n",
    "    print(response_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bcd35983",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "01240e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataCollatorForLanguageModeling\n",
    "data_collator = DataCollatorWithPadding(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False  # we're doing causal LM, not masked LM\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0d53ae37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 241\n",
       "})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ff1dedf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cfee853e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94c74025b5e84146bc26fdbcd59bf900",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating train dataset:   0%|          | 0/2166 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39d4924d3b894923a2401df5077a7e28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating eval dataset:   0%|          | 0/241 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">swanlab</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">:</span> swanlab version <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.6</span>.<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span> is available!  Upgrade: `pip install -U swanlab`\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;34mswanlab\u001b[0m\u001b[1;39m:\u001b[0m swanlab version \u001b[1;36m0.6\u001b[0m.\u001b[1;36m10\u001b[0m is available!  Upgrade: `pip install -U swanlab`\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">swanlab</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">:</span> Tracking run with swanlab version <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.6</span>.<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;34mswanlab\u001b[0m\u001b[1;39m:\u001b[0m Tracking run with swanlab version \u001b[1;36m0.6\u001b[0m.\u001b[1;36m9\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">swanlab</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">:</span> Run data will be saved locally in \n",
       "<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">/home/luke/distributed_machine_learning/Qwen3-Medical-SFT/swanlog/run-20250915_095044-z26s1shexbxf7ji1qoqcd</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;34mswanlab\u001b[0m\u001b[1;39m:\u001b[0m Run data will be saved locally in \n",
       "\u001b[1;35m/home/luke/distributed_machine_learning/Qwen3-Medical-SFT/swanlog/run-20250915_095044-z26s1shexbxf7ji1qoqcd\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">swanlab</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">:</span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\"> </span>👋 Hi <span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">swanluker</span>,welcome to swanlab!\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;34mswanlab\u001b[0m\u001b[1;39m:\u001b[0m\u001b[1;34m \u001b[0m👋 Hi \u001b[1;39mswanluker\u001b[0m,welcome to swanlab!\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">swanlab</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">:</span> Syncing run <span style=\"color: #808000; text-decoration-color: #808000\">qwen3-1.7B-jupyter-trl-v2</span> to the cloud\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;34mswanlab\u001b[0m\u001b[1;39m:\u001b[0m Syncing run \u001b[33mqwen3-1.7B-jupyter-trl-v2\u001b[0m to the cloud\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">swanlab</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">:</span> 🏠 View project at <span style=\"color: #000080; text-decoration-color: #000080; text-decoration: underline\">https://swanlab.cn/@swanluker/qwen3-sft-medical</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;34mswanlab\u001b[0m\u001b[1;39m:\u001b[0m 🏠 View project at \u001b[4;34mhttps://swanlab.cn/@swanluker/qwen3-sft-medical\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">swanlab</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">:</span> 🚀 View run at <span style=\"color: #000080; text-decoration-color: #000080; text-decoration: underline\">https://swanlab.cn/@swanluker/qwen3-sft-medical/runs/z26s1shexbxf7ji1qoqcd</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;34mswanlab\u001b[0m\u001b[1;39m:\u001b[0m 🚀 View run at \u001b[4;34mhttps://swanlab.cn/@swanluker/qwen3-sft-medical/runs/z26s1shexbxf7ji1qoqcd\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<!DOCTYPE html>\n",
       "<html lang=\"en\">\n",
       "<head>\n",
       "    <meta charset=\"UTF-8\">\n",
       "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
       "    <title>Show Iframe</title>\n",
       "    \n",
       "        <script>\n",
       "            function showIframe() {\n",
       "                var iframeHtml = '<iframe src=\"https://swanlab.cn/@swanluker/qwen3-sft-medical/runs/z26s1shexbxf7ji1qoqcd\" width=100% height=\"600\" frameborder=\"no\"></iframe>';\n",
       "                document.getElementById('iframeContainer').innerHTML = iframeHtml;\n",
       "            }\n",
       "        </script>\n",
       "        \n",
       "</head>\n",
       "<body>\n",
       "    <style>\n",
       "        .interactive-button {\n",
       "            display: flex;\n",
       "            align-items: center;\n",
       "            height: 36px;\n",
       "            border: 0px;\n",
       "            background-color: #2c8f63;\n",
       "            color: white;\n",
       "            padding: 10px 20px;\n",
       "            transition: background-color 0.3s, transform 0.2s;\n",
       "        }\n",
       "\n",
       "        .interactive-button:hover {\n",
       "            background-color: #5cab87;\n",
       "            cursor: pointer;\n",
       "        }\n",
       "\n",
       "        .interactive-button:active { background-color: #217952; transform: scale(0.96); } </style> <br> <button \n",
       "        onclick=\"showIframe()\" class=\"interactive-button\"> <svg style=\"height: 16px; margin-right: 8px;\" viewBox=\"0 0 \n",
       "        46 46\" fill=\"none\"> <path d=\"M10.8439 21.1974C10.6414 21.2854 10.4477 21.3925 10.2655 21.5173L10.2069 \n",
       "        21.5652C10.1839 21.58 10.1625 21.5969 10.1429 21.6159C6.29135 24.6118 4.22831 29.4416 5.32646 34.282C5.94656 \n",
       "        37.0577 7.50461 39.5348 9.73801 41.2958C11.9714 43.0568 14.7436 43.994 17.5874 43.9495H18.0219C19.8864 \n",
       "        43.8697 21.7087 43.3694 23.3526 42.486C24.9964 41.6026 26.4193 40.3589 27.5147 38.848C28.61 37.3371 29.3496 \n",
       "        35.598 29.678 33.761C30.0065 31.9239 29.9153 30.0363 29.4112 28.2395C28.9181 26.4723 27.8919 24.8437 26.9937 \n",
       "        23.2551C25.4158 20.4653 23.8343 17.6764 22.2492 14.8884C21.7801 14.0647 21.3057 13.2465 20.8419 \n",
       "        12.4228C20.2315 11.3353 19.2746 10.1519 19.224 8.86183C19.1733 7.57176 20.2235 6.32701 21.5082 \n",
       "        6.07912C23.9284 5.61801 25.0639 8.24078 25.0693 8.23812C25.363 8.94035 25.9123 9.50489 26.6063 \n",
       "        9.81764C27.3002 10.1304 28.087 10.168 28.8077 9.92298C29.5283 9.67791 30.1291 9.1684 30.4885 8.49743C30.8479 \n",
       "        7.82646 30.9392 7.04405 30.7439 6.30835C30.1514 4.37314 28.9133 2.69953 27.2363 1.56656C25.7615 0.511704 \n",
       "        23.9847 -0.0372109 22.1719 0.00195984C20.9049 0.00893199 19.6532 0.27989 18.4967 0.797557C17.3402 1.31522 \n",
       "        16.3043 2.06823 15.4551 3.00856C14.49 4.08707 13.7984 5.38193 13.4389 6.78385C13.0794 8.18576 13.0624 9.6536 \n",
       "        13.3894 11.0635C13.52 11.593 13.6984 12.1095 13.9225 12.6067C14.5595 14.0514 15.4951 15.3681 16.284 \n",
       "        16.7355C17.2525 18.4147 18.2209 20.0948 19.1893 21.7758C20.1578 23.4568 21.1351 25.1449 22.1213 \n",
       "        26.8401C22.9209 28.2421 23.7925 29.4682 23.8805 31.1528C23.9175 32.0513 23.7682 32.9479 23.4419 \n",
       "        33.7859C23.1156 34.6239 22.6194 35.3854 21.9845 36.0223C21.3496 36.6592 20.5897 37.1578 19.7527 \n",
       "        37.4868C18.9157 37.8157 18.0196 37.9678 17.121 37.9336C14.0024 37.7923 11.6488 35.4814 11.1744 32.4588C10.58 \n",
       "        28.6419 13.552 26.5469 13.552 26.5469C14.1782 26.1785 14.6497 25.5955 14.8791 24.906C15.1084 24.2166 15.0801 \n",
       "        23.4673 14.7993 22.7971C14.5186 22.127 14.0044 21.5813 13.3521 21.2611C12.6998 20.941 11.9536 20.8682 11.2517 \n",
       "        21.0561C11.1174 21.0939 10.9856 21.1402 10.8572 21.1947\" fill=\"white\" /> <path d=\"M42.8101 31.5968C42.8109 \n",
       "        30.5198 42.7218 29.4445 42.5435 28.3823C42.2663 26.7069 41.7464 25.0808 41.0002 23.5552C40.5524 22.6463 \n",
       "        39.9874 21.7374 39.1024 21.2417C38.6593 20.9919 38.1589 20.8617 37.6502 20.8639C37.1416 20.8661 36.6423 \n",
       "        21.0006 36.2013 21.2541C35.7604 21.5077 35.393 21.8716 35.1352 22.3101C34.8775 22.7485 34.7382 23.2466 \n",
       "        34.7312 23.7552C34.7072 24.8773 35.3149 25.8875 35.768 26.9217C36.5212 28.6453 36.8623 30.5208 36.7642 \n",
       "        32.3993C36.6661 34.2777 36.1315 36.1075 35.2029 37.7433C35.146 37.8404 35.0952 37.941 35.051 38.0445C34.8623 \n",
       "        38.4842 34.7635 38.9573 34.7605 39.4358C34.7802 40.1222 35.0356 40.7808 35.4835 41.3011C35.9315 41.8214 \n",
       "        36.5449 42.1717 37.2207 42.2932C38.8759 42.589 40.1899 41.347 40.8856 39.9609C42.1643 37.3589 42.823 34.4961 \n",
       "        42.8101 31.5968Z\" fill=\"white\" /> <path d=\"M28.2309 11.8938C28.1761 11.9043 28.1218 11.9176 28.0683 \n",
       "        11.9338C27.9593 11.9642 27.8611 12.0249 27.7851 12.1088C27.7091 12.1928 27.6584 12.2965 27.6389 \n",
       "        12.408C27.6193 12.5195 27.6318 12.6343 27.6748 12.7391C27.7178 12.8438 27.7895 12.9343 27.8818 \n",
       "        12.9999C29.2375 14.0252 30.3809 15.3043 31.2482 16.7662C31.4838 17.1677 31.6888 17.5865 31.8612 \n",
       "        18.0189C32.0052 18.3921 32.1971 18.8799 32.6822 18.8532C33.0607 18.8346 33.2153 18.512 33.3192 \n",
       "        18.1895C33.8137 16.5125 33.9678 14.7534 33.7723 13.0159C33.6331 12.0693 33.4155 11.1359 33.122 \n",
       "        10.2252C33.0775 10.0047 32.9744 9.80029 32.8235 9.6335C32.7273 9.54627 32.6054 9.49262 32.4761 9.4806C32.3468 \n",
       "        9.46859 32.2171 9.49886 32.1065 9.56687C32.0016 9.65188 31.9115 9.75365 31.8399 9.86806C31.3956 10.4658 \n",
       "        30.825 10.9581 30.1687 11.3101C29.8377 11.4861 29.4893 11.6272 29.1292 11.7312C28.828 11.8192 28.5215 11.8325 \n",
       "        28.2309 11.8938Z\" fill=\"white\" /> </svg> Display SwanLab Board </button> <br> <div \n",
       "        id=\"iframeContainer\"></div> </body> </html>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='722' max='722' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [722/722 46:24, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.172700</td>\n",
       "      <td>1.729462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.106700</td>\n",
       "      <td>1.695816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.213200</td>\n",
       "      <td>1.668192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.234800</td>\n",
       "      <td>1.598775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.246700</td>\n",
       "      <td>1.565442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.213800</td>\n",
       "      <td>1.516079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.209000</td>\n",
       "      <td>1.493954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.240900</td>\n",
       "      <td>1.457687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>0.285500</td>\n",
       "      <td>1.438660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.345200</td>\n",
       "      <td>1.423140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>0.159500</td>\n",
       "      <td>1.405936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.259700</td>\n",
       "      <td>1.384612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>0.226500</td>\n",
       "      <td>1.375205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>0.195700</td>\n",
       "      <td>1.361892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.244800</td>\n",
       "      <td>1.346868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.117200</td>\n",
       "      <td>1.335555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>510</td>\n",
       "      <td>0.273400</td>\n",
       "      <td>1.325415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>0.220900</td>\n",
       "      <td>1.318762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>570</td>\n",
       "      <td>0.309600</td>\n",
       "      <td>1.312527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.192200</td>\n",
       "      <td>1.304926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>630</td>\n",
       "      <td>0.284600</td>\n",
       "      <td>1.301968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>660</td>\n",
       "      <td>0.214500</td>\n",
       "      <td>1.299975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>690</td>\n",
       "      <td>0.113000</td>\n",
       "      <td>1.299498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>0.230400</td>\n",
       "      <td>1.299406</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luke/miniconda3/envs/openr1/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'SSLError(SSLEOFError(8, 'EOF occurred in violation of protocol (_ssl.c:992)'))': /api/house/metrics\n",
      "WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'SSLError(SSLEOFError(8, 'EOF occurred in violation of protocol (_ssl.c:992)'))': /api/house/metrics\n",
      "WARNING:urllib3.connectionpool:Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'SSLError(SSLEOFError(8, 'EOF occurred in violation of protocol (_ssl.c:992)'))': /api/house/metrics\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">swanlab</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">:</span> network error, swanlab will resume uploads when the network improves\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;33mswanlab\u001b[0m\u001b[1;39m:\u001b[0m network error, swanlab will resume uploads when the network improves\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luke/miniconda3/envs/openr1/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=722, training_loss=0.20251906229766123, metrics={'train_runtime': 2792.2015, 'train_samples_per_second': 1.551, 'train_steps_per_second': 0.259, 'total_flos': 3.5627337800073216e+16, 'train_loss': 0.20251906229766123})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# args = TrainingArguments(\n",
    "#     output_dir=\"./output/Qwen3-1.7B\",\n",
    "#     per_device_train_batch_size=1,\n",
    "#     per_device_eval_batch_size=1,\n",
    "#     gradient_accumulation_steps=4,\n",
    "#     eval_strategy=\"steps\",\n",
    "#     eval_steps=100,\n",
    "#     logging_steps=10,\n",
    "#     num_train_epochs=2,\n",
    "#     save_steps=400,\n",
    "#     learning_rate=1e-4,\n",
    "#     save_on_each_node=True,\n",
    "#     gradient_checkpointing=True,\n",
    "#     report_to=\"swanlab\",\n",
    "#     run_name=\"qwen3-1.7B\",\n",
    "# )\n",
    "\n",
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     args=args,\n",
    "#     train_dataset=train_dataset,\n",
    "#     eval_dataset=eval_dataset,\n",
    "#     data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer, padding=True),\n",
    "# )\n",
    "\n",
    "\n",
    "\n",
    "args=TrainingArguments(\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=1,\n",
    "    # Use num_train_epochs = 1, warmup_ratio for full training runs!\n",
    "    num_train_epochs=2,\n",
    "    warmup_steps=5,\n",
    "    #max_steps=60,\n",
    "    learning_rate=1e-4,\n",
    "    gradient_checkpointing=True,\n",
    "    logging_steps=1,\n",
    "    \n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=30,\n",
    "\n",
    "    seed=3407,\n",
    "    output_dir=\"outputs\",\n",
    "    report_to=\"swanlab\",\n",
    "    run_name=\"qwen3-1.7B-jupyter-trl-v2\"\n",
    "    \n",
    "    \n",
    "    #    optim=\"adamw_8bit\",\n",
    "    #    weight_decay=0.01,\n",
    "    #    lr_scheduler_type=\"linear\",\n",
    "    #     fp16=not is_bfloat16_supported(),\n",
    "    #     bf16=is_bfloat16_supported(),\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=data_collator,\n",
    "#    dataset_text_field=\"text\",\n",
    "#    max_seq_length=max_seq_length,\n",
    "#    tokenizer=tokenizer,    \n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# 用测试集的前3条，主观看模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4ef8ce26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 2166\n",
       "})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8c451e2e-8542-404c-9eab-8fd610bf8e12",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luke/miniconda3/envs/openr1/lib/python3.11/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Question: 1895年德国物理学教授伦琴的发现对医学影像学的发展有何具体影响？请从技术进步、学科建立和临床应用三个方面进行分析。\n",
      "\n",
      "    LLM:<think>嗯，用户问的是1895年伦琴发现X光对医学影像学的影响，需要从技术进步、学科建立和临床应用三个角度分析。首先，我得回忆一下伦琴是谁，他是德国的物理学家，对吧？他可能是在研究X射线的时候发现了这个现象。\n",
      "\n",
      "首先，技术进步方面，X光的发现肯定是技术上的突破。X射线属于电离辐射，能穿透物质，这比传统的可见光或者紫外线强很多。那为什么这能成为医学影像学的基础呢？可能因为它的穿透性，使得医生可以不用切开病人，就能看到内部结构，比如骨骼或者器官。这应该让诊断更快、更安全，减少了组织损伤，所以技术上是革命性的。\n",
      "\n",
      "然后是学科建立。医学影像学作为一门独立的学科，应该是在伦琴之后才出现的。之前可能医学诊断更多依赖解剖学知识，而X光的出现让影像学成为医学的一部分，比如放射科，专门处理影像检查。可能当时医学界开始将影像学作为诊断的重要手段，而不是仅靠解剖学或病理学。\n",
      "\n",
      "临床应用方面，X光的应用可能促进了后续的诊断技术，比如CT、MRI、超声这些，这些都是在X光的基础上发展起来的。比如CT扫描就是通过多角度的X光成像，然后结合计算机技术分析图像，这比单一的X光片更全面。另外，X光在临床中的普及可能改变了患者对疾病的诊断方式，比如不再需要依赖活检或手术，而是通过影像学直接观察，减少患者的痛苦和风险。\n",
      "\n",
      "不过，用户的问题里可能还希望更深入的分析，比如伦琴如何具体推动这些发展。比如他可能在研究X射线时，意识到它的临床应用潜力，或者他可能在教学或临床实践中应用了这些技术，从而促进了学科的发展。另外，技术进步如何影响后续的发展，比如X光设备的改进，或者成像技术的演变，比如从荧光屏到数字图像技术，这些都是技术进步的延续。\n",
      "\n",
      "还要注意用户可能作为医学生或医学生，想了解医学史，所以需要强调伦琴的贡献不仅停留在技术层面，还影响了学科的建立和临床应用，说明其多方面的影响。可能需要检查是否有其他因素，比如伦琴是否参与了其他医学领域的贡献，或者他的教学和临床实践如何影响了医学影像学的形成。\n",
      "\n",
      "总结下来，回答的结构应该是先分点，每个部分详细说明，用例子和逻辑连接起来，确保覆盖技术、学科、临床应用，并且突出伦琴的直接和间接影响。可能需要确认每个点的具体时间线和影响顺序，比如技术进步是直接结果，学科建立是更长期的影响，临床应用是实际应用的结果。\n",
      "</think> \n",
      " 1895年，德国物理学教授威廉·伦琴在研究X射线时，不仅揭示了X射线的特性，还将其应用于医学领域，这标志着医学影像学的正式建立。X射线技术的进步为医学诊断提供了新的可能性，减少了对活检或手术的依赖，提高了诊断的准确性和安全性。此外，X射线技术的发现促进了医学影像学的学科发展，使得影像学成为医学诊断的重要组成部分。随着X射线技术的进一步应用，它在临床医学中的应用也日益广泛，推动了后续更先进的影像诊断技术的发展。这些变化都极大地改善了患者的治疗体验，提高了医疗诊断的效率和准确性。伦琴的贡献不仅限于技术层面，还为医学影像学的学科建立和临床应用奠定了基础。\n",
      "    \n",
      "\n",
      "    Question: 医生，我最近胃部不适，听说有几种抗溃疡药物可以治疗，您能详细介绍一下这些药物的分类、作用机制以及它们是如何影响胃黏膜的保护与损伤平衡的吗？\n",
      "\n",
      "    LLM:<think>嗯，用户问的是抗溃疡药物的分类、作用机制，还有它们如何影响胃黏膜的保护和损伤平衡。首先，我需要回忆一下抗溃疡药物的基本分类。记得以前学过，抗溃疡药物主要分为几大类，比如H2受体拮抗剂、PPI，还有抗酸药。不过可能还有其他类别，比如抗生素、免疫调节剂之类的，但用户的问题里可能不需要太详细，所以先按主要类别来。\n",
      "\n",
      "接下来是每种药物的作用机制。H2受体拮抗剂比如奥美拉唑，我记得它们通过抑制胃酸分泌，减少对黏膜的损伤。而PPI是质子泵抑制剂，作用更彻底，能抑制H+的分泌，减少胃酸对黏膜的侵蚀。抗酸药比如铝碳酸镁，直接中和胃酸，但可能在长期使用后会有耐药性，所以可能不是首选。\n",
      "\n",
      "然后是关于它们如何影响胃黏膜的保护和损伤平衡。需要解释这些药物如何调节胃酸分泌，从而保护黏膜。比如，H2受体拮抗剂和PPI减少胃酸分泌，从而减少黏膜受损；而抗酸药虽然直接中和，但可能在长期使用后导致黏膜受损，因为胃酸分泌减少后，黏膜可能无法及时修复。\n",
      "\n",
      "用户提到自己有胃部不适，可能需要具体例子，比如提到的奥美拉唑和PPI，以及抗酸药如铝碳酸镁。需要说明这些药物如何通过不同的机制起作用，以及它们的优缺点。比如，H2受体拮抗剂和PPI可能更适合长期使用，而抗酸药可能短期有效但耐药性。\n",
      "\n",
      "可能还需要考虑药物相互作用，比如长期使用抗酸药是否会影响PPI的效果？或者H2受体拮抗剂是否会影响其他药物的代谢？不过用户的问题可能不需要深入这些细节，但答案里已经提到了可能的相互作用，所以需要简要提及。\n",
      "\n",
      "另外，用户可能想知道这些药物如何具体影响胃黏膜的结构和功能，比如改变胃酸分泌的量，减少黏膜接触，从而减少炎症或溃疡。同时，黏膜屏障的修复可能通过药物促进修复过程，比如H2受体拮抗剂可能促进黏膜细胞的修复。\n",
      "\n",
      "需要确保每个药物的作用机制和对黏膜的影响都解释清楚，比如为什么抗酸药虽然能中和胃酸，但可能促进黏膜损伤，因为胃酸分泌减少后，黏膜可能无法及时修复，或者胃酸减少后，黏膜的酸性环境变缓，导致黏膜损伤。\n",
      "\n",
      "可能还要比较不同药物的优缺点，比如H2受体拮抗剂和PPI在胃酸分泌抑制上的效果差异，或者抗酸药的短期效果。这样用户能根据自己的情况选择药物。\n",
      "\n",
      "总结下来，结构应该是先分类，再分别解释每种药物的作用机制，然后说明它们如何影响黏膜的保护和损伤平衡，最后可能提到药物相互作用和长期使用的影响。确保语言通俗易懂，避免过于专业的术语，但用户是医生，可能需要一定的专业性，所以需要平衡。\n",
      "</think> \n",
      " 当然可以。抗溃疡药物主要分为几类，包括H2受体拮抗剂、PPI和抗酸药。H2受体拮抗剂如奥美拉唑，它们通过减少胃酸分泌来保护胃黏膜，防止胃酸对黏膜的侵蚀。PPI，如质子泵抑制剂，作用于胃黏膜上的H+泵，进一步减少胃酸分泌，增强黏膜的保护作用。抗酸药如铝碳酸镁，虽然能快速中和胃酸，但可能对胃黏膜的保护作用有限，因为长期使用抗酸药可能会导致黏膜的损伤，尤其是在胃酸分泌减少的情况下。每种药物都有其特点和适应症，医生会根据您的具体情况来选择最合适的药物。此外，长期使用抗酸药可能会引起胃酸分泌减少，这可能会影响黏膜的保护作用，因此需要谨慎使用。希望这些信息对您有所帮助。如果您有任何疑问，欢迎随时向我提问。\n",
      "    \n",
      "\n",
      "    Question: 我最近被诊断出患有淋巴瘤，医生提到这可能导致发热。请问这是由于淋巴瘤组织的坏死和细胞破坏引起的吗？如果是，具体机制是什么？\n",
      "\n",
      "    LLM:<think>嗯，用户问的是淋巴瘤导致发热的具体机制，还提到自己被诊断出淋巴瘤，医生说可能发热。首先，我需要回忆一下淋巴瘤的基本知识。淋巴瘤是一种起源于淋巴细胞的恶性肿瘤，通常发生在年轻人身上，尤其是15到30岁之间。发热是淋巴瘤常见的症状之一，但具体原因是什么呢？\n",
      "\n",
      "首先，用户提到的坏死和细胞破坏应该是指淋巴瘤细胞在肿瘤生长过程中，可能因为增殖过度导致坏死。坏死会导致细胞内容物泄漏，可能刺激免疫系统。然后，细胞破坏可能指的是肿瘤细胞本身被破坏，释放的物质进入体内，引发炎症反应。\n",
      "\n",
      "接下来，我需要详细解释坏死的具体机制。坏死可能发生在肿瘤细胞快速增殖时，导致细胞结构破坏，比如细胞膜破裂，细胞内容物如细胞质、细胞核等被释放。这些物质包括细胞内的物质，比如细胞内的物质可能被释放到血液中，进而刺激免疫系统。\n",
      "\n",
      "然后，细胞破坏可能是指肿瘤细胞被破坏，比如凋亡或坏死后的细胞碎片进入血液。这些细胞碎片可能含有细胞内的物质，比如细胞质中的细胞器，或者细胞核中的DNA等，这些物质被释放到血液中，引发炎症反应。\n",
      "\n",
      "炎症反应是发热的核心机制。当这些物质被释放到血液中后，会被免疫系统识别为异物，引发炎症反应。炎症反应会激活免疫细胞，比如巨噬细胞、中性粒细胞等，导致白细胞和淋巴细胞的活化，释放更多的炎症介质，比如内源性致热原（IL-1、IL-2、TNF-α等），这些物质刺激血管收缩，促进血管通透性，导致更多白蛋白进入血管，形成血栓，从而引起发热。\n",
      "\n",
      "另外，发热是免疫系统的一种防御反应，但在这个情况下，可能由于过度激活导致发热。需要区分发热是正常的免疫反应还是过度反应，但淋巴瘤患者可能因为免疫系统的异常激活而出现发热。\n",
      "\n",
      "可能用户还想知道为什么淋巴瘤患者更容易发热，或者发热对治疗的影响。但根据问题，重点在机制，所以需要集中在坏死和细胞破坏如何引发炎症反应。\n",
      "\n",
      "需要确认坏死是否真的是淋巴瘤的主要机制，以及炎症反应是否是主要途径。可能还要考虑其他可能的机制，比如细胞因子的释放，或者免疫细胞的活化，但根据答案部分，重点在炎症反应。\n",
      "\n",
      "总结的话，坏死和细胞破坏导致细胞内容物释放，引发炎症反应，导致发热。需要将这些点有条理地组织起来，确保用户理解每个环节的关系。\n",
      "</think> \n",
      " 是的，淋巴瘤可能导致发热。这种发热主要是由于淋巴瘤组织的坏死和细胞破坏引起的。当淋巴瘤细胞增殖过度时，它们可能会发生坏死，导致细胞内容物的释放，这些物质可以刺激免疫系统，引发炎症反应。炎症反应是发热的主要机制，当免疫系统被激活时，它会通过释放炎症介质来应对刺激，这可能导致发热。如果发热过高，可能会引起全身性的炎症反应综合征，进而影响患者的健康。建议您密切监测发热情况，并及时与医生沟通，以便采取相应的治疗措施。同时，保持良好的心态对疾病的康复也非常重要，可以适当进行一些放松和愉悦的活动。希望这些信息对您有所帮助。\n",
      "    \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">swanlab</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">:</span> 🏠 View project at <span style=\"color: #000080; text-decoration-color: #000080; text-decoration: underline\">https://swanlab.cn/@swanluker/qwen3-sft-medical</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;34mswanlab\u001b[0m\u001b[1;39m:\u001b[0m 🏠 View project at \u001b[4;34mhttps://swanlab.cn/@swanluker/qwen3-sft-medical\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">swanlab</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">:</span> 🚀 View run at <span style=\"color: #000080; text-decoration-color: #000080; text-decoration: underline\">https://swanlab.cn/@swanluker/qwen3-sft-medical/runs/z26s1shexbxf7ji1qoqcd</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;34mswanlab\u001b[0m\u001b[1;39m:\u001b[0m 🚀 View run at \u001b[4;34mhttps://swanlab.cn/@swanluker/qwen3-sft-medical/runs/z26s1shexbxf7ji1qoqcd\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "test_df = pd.read_json(test_jsonl_new_path, lines=True)[:3]\n",
    "\n",
    "test_text_list = []\n",
    "\n",
    "for index, row in test_df.iterrows():\n",
    "    instruction = row['instruction']\n",
    "    input_value = row['input']\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": f\"{instruction}\"},\n",
    "        {\"role\": \"user\", \"content\": f\"{input_value}\"}\n",
    "    ]\n",
    "\n",
    "    response = predict(messages, model, tokenizer)\n",
    "\n",
    "    response_text = f\"\"\"\n",
    "    Question: {input_value}\n",
    "\n",
    "    LLM:{response}\n",
    "    \"\"\"\n",
    "    \n",
    "    test_text_list.append(swanlab.Text(response_text))\n",
    "    print(response_text)\n",
    "\n",
    "swanlab.log({\"Prediction\": test_text_list})\n",
    "\n",
    "swanlab.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2df6f8c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
