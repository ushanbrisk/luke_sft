{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75b09255-c46f-42b2-9e92-5e64861efa43",
   "metadata": {},
   "source": [
    "# Qwen3微调实战：医疗R1推理风格聊天\n",
    "\n",
    "[![](https://raw.githubusercontent.com/SwanHubX/assets/main/badge1.svg)](https://swanlab.cn/@ZeyiLin/qwen3-sft-medical/overview)\n",
    "\n",
    "- **Github**: [Qwen3-Medical-SFT](https://github.com/Zeyi-Lin/Qwen3-Medical-SFT)\n",
    "- **基础模型**：[Qwen3-1.7B](https://modelscope.cn/models/Qwen/Qwen3-1.7B/summary)\n",
    "- **微调后模型**：[Qwen3-1.7b-Medical-R1-sft](https://modelscope.cn/models/testUser/Qwen3-1.7b-Medical-R1-sft/summary)\n",
    "- **数据集**：[delicate_medical_r1_data](https://modelscope.cn/datasets/krisfu/delicate_medical_r1_data)\n",
    "- **SwanLab**：[qwen3-sft-medical](https://swanlab.cn/@ZeyiLin/qwen3-sft-medical/runs/agps0dkifth5l1xytcdyk/chart)\n",
    "- **微调方式**：全参数微调、LoRA微调\n",
    "- **推理风格**：R1推理风格\n",
    "- **算力要求**：\n",
    "  - **全参数微调**：32GB显存\n",
    "  - **LoRA微调**：28GB显存\n",
    "- **图文教程**：[Qwen3大模型微调入门实战（完整代码）](https://zhuanlan.zhihu.com/p/1903848838214705484)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b4fa4a-03a4-4bdb-b593-b50d2ebbb931",
   "metadata": {},
   "source": [
    "## 1. 安装环境"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16fc9e42",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luke/miniconda3/envs/openr1/lib/python3.11/site-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n",
      "/home/luke/miniconda3/envs/openr1/lib/python3.11/site-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from modelscope import snapshot_download, AutoTokenizer\n",
    "from transformers import AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForSeq2Seq\n",
    "\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "\n",
    "import os\n",
    "import swanlab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78d71b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "PROMPT = \"你是一个医学专家，你需要根据用户的问题，给出带有思考的回答。\"\n",
    "MAX_LENGTH = 2048\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e21e0fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def dataset_jsonl_transfer(origin_path, new_path):\n",
    "    \"\"\"\n",
    "    将原始数据集转换为大模型微调所需数据格式的新数据集\n",
    "    \"\"\"\n",
    "    messages = []\n",
    "\n",
    "    # 读取旧的JSONL文件\n",
    "    with open(origin_path, \"r\") as file:\n",
    "        for line in file:\n",
    "            # 解析每一行的json数据\n",
    "            data = json.loads(line)\n",
    "            input = data[\"question\"]\n",
    "            think = data[\"think\"]\n",
    "            answer = data[\"answer\"]\n",
    "            output = f\"<think>{think}</think> \\n {answer}\"\n",
    "            message = {\n",
    "                \"instruction\": PROMPT,\n",
    "                \"input\": f\"{input}\",\n",
    "                \"output\": output,\n",
    "            }\n",
    "            messages.append(message)\n",
    "\n",
    "    # 保存重构后的JSONL文件\n",
    "    with open(new_path, \"w\", encoding=\"utf-8\") as file:\n",
    "        for message in messages:\n",
    "            file.write(json.dumps(message, ensure_ascii=False) + \"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88df11b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4b88f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_func2(example):\n",
    "    \n",
    "\n",
    "    instruction = example['instruction']\n",
    "\n",
    "    \n",
    "    input1 = example['input']\n",
    "    \n",
    "    output = example['output']\n",
    "\n",
    "    messages = [\n",
    "        {'role': 'system', 'content': f\"{PROMPT}\"},\n",
    "        {'role': 'user', 'content': input1},\n",
    "        {'role': 'assistant', 'content': output},\n",
    "                ]\n",
    "\n",
    "\n",
    "\n",
    "    text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=False,\n",
    "    truncate=True,\n",
    "#     return_tensors='pt',\n",
    "#     enable_thinking=False\n",
    "\n",
    "    )\n",
    "#     example['text'] = text\n",
    "    return {\"text\":text}\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07aa602b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def process_func(example):\n",
    "    \"\"\"\n",
    "    将数据集进行预处理\n",
    "    \"\"\" \n",
    "    input_ids, attention_mask, labels = [], [], []\n",
    "    instruction = tokenizer(\n",
    "        f\"<|im_start|>system\\n{PROMPT}<|im_end|>\\n<|im_start|>user\\n{example['input']}<|im_end|>\\n<|im_start|>assistant\\n\",\n",
    "        add_special_tokens=False,\n",
    "    )\n",
    "    response = tokenizer(f\"{example['output']}\", add_special_tokens=False)\n",
    "    input_ids = instruction[\"input_ids\"] + response[\"input_ids\"] + [tokenizer.pad_token_id]\n",
    "    attention_mask = (\n",
    "        instruction[\"attention_mask\"] + response[\"attention_mask\"] + [1]\n",
    "    )\n",
    "    labels = [-100] * len(instruction[\"input_ids\"]) + response[\"input_ids\"] + [tokenizer.pad_token_id]\n",
    "    if len(input_ids) > MAX_LENGTH:  # 做一个截断\n",
    "        input_ids = input_ids[:MAX_LENGTH]\n",
    "        attention_mask = attention_mask[:MAX_LENGTH]\n",
    "        labels = labels[:MAX_LENGTH]\n",
    "    return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"labels\": labels}   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "06ff1faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def predict(messages, model, tokenizer):\n",
    "    device = model.device\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n",
    "\n",
    "    generated_ids = model.generate(\n",
    "        model_inputs.input_ids,\n",
    "        max_new_tokens=MAX_LENGTH,\n",
    "    )\n",
    "    generated_ids = [\n",
    "        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "\n",
    "    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "11dfc315",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Transformers加载模型权重\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/ssd2/output_test_202509152109_debug\", use_fast=False, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"/ssd2/output_test_202509152109_debug\", device_map=\"cuda:0\", torch_dtype=torch.bfloat16)\n",
    "model.enable_input_require_grads()  # 开启梯度检查点时，要执行该方法\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0de533a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen2ForCausalLM(\n",
       "  (model): Qwen2Model(\n",
       "    (embed_tokens): Embedding(151936, 896, padding_idx=151645)\n",
       "    (layers): ModuleList(\n",
       "      (0-23): 24 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2Attention(\n",
       "          (q_proj): Linear(in_features=896, out_features=896, bias=True)\n",
       "          (k_proj): Linear(in_features=896, out_features=128, bias=True)\n",
       "          (v_proj): Linear(in_features=896, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=896, out_features=896, bias=False)\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
       "          (up_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
       "          (down_proj): Linear(in_features=4864, out_features=896, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "    (rotary_emb): Qwen2RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=896, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "48ee02c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen2ForCausalLM(\n",
       "  (model): Qwen2Model(\n",
       "    (embed_tokens): Embedding(151936, 896, padding_idx=151645)\n",
       "    (layers): ModuleList(\n",
       "      (0-23): 24 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2Attention(\n",
       "          (q_proj): Linear(in_features=896, out_features=896, bias=True)\n",
       "          (k_proj): Linear(in_features=896, out_features=128, bias=True)\n",
       "          (v_proj): Linear(in_features=896, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=896, out_features=896, bias=False)\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
       "          (up_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
       "          (down_proj): Linear(in_features=4864, out_features=896, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "    (rotary_emb): Qwen2RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=896, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f6caa02a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "988dfa77",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 加载、处理数据集和测试集\n",
    "train_dataset_path = \"train.jsonl\"\n",
    "test_dataset_path = \"val.jsonl\"\n",
    "\n",
    "train_jsonl_new_path = \"train_format.jsonl\"\n",
    "test_jsonl_new_path = \"val_format.jsonl\"\n",
    "\n",
    "if not os.path.exists(train_jsonl_new_path):\n",
    "    dataset_jsonl_transfer(train_dataset_path, train_jsonl_new_path)\n",
    "if not os.path.exists(test_jsonl_new_path):\n",
    "    dataset_jsonl_transfer(test_dataset_path, test_jsonl_new_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "22033dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 得到训练集\n",
    "train_df = pd.read_json(train_jsonl_new_path, lines=True)\n",
    "train_ds = Dataset.from_pandas(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "53ab10ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = process_func2(train_ds[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0c808a01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': '<|im_start|>system\\n你是一个医学专家，你需要根据用户的问题，给出带有思考的回答。<|im_end|>\\n<|im_start|>user\\n医生，我想了解一下如何测定盐酸甲氧明注射液的含量，具体步骤是什么样的？<|im_end|>\\n<|im_start|>assistant\\n<think>嗯，用户问的是如何测定盐酸甲氧明注射液的含量，具体步骤是什么。首先，我需要回忆一下相关的分析方法，特别是药典中常用的方法。盐酸甲氧明属于药物分析中的常见成分，通常可能用紫外分光光度法或者HPLC之类的。\\n\\n首先，用户提到的是紫外-可见分光光度法，这应该是比较直接的方法。那步骤的话，首先应该考虑样品的制备。因为注射液浓度可能较高，需要稀释到合适的浓度范围，使得吸光度在仪器的线性范围内。比如，先取一定量的注射液，然后稀释到某个体积，比如250毫升的量瓶，这样可能是一个步骤。\\n\\n接下来，可能需要进一步稀释，因为第一次稀释后的溶液可能吸光度还是太高或者太低。比如，从第一次稀释后的溶液中再取一部分，比如10毫升，放到另一个量瓶，比如100毫升，再次稀释。这样两次稀释可以得到合适的浓度。\\n\\n然后，测定吸光度的时候，波长的选择很重要。盐酸甲氧明的最大吸收波长应该查文献或者药典，比如问题中的答案提到的是290纳米，这可能来自药典的规定。需要确认这个波长是否正确，以及吸收系数是否准确。比如，吸收系数137是否对应C1H1NO3·HCl的计算，可能需要根据分子量和浓度来计算，但用户可能不需要详细计算，只需要步骤。\\n\\n另外，步骤中的精密量取需要注意，比如使用移液管或者自动进样器，确保量取的准确性。摇匀也是关键步骤，确保溶液均匀，避免局部浓度不均导致误差。\\n\\n可能还需要考虑空白对照，比如用水代替样品，进行同样的操作，以扣除背景吸光度。不过答案中没有提到，可能在标准方法中已经包含，或者步骤简化了。但作为详细步骤，可能需要补充这一点。\\n\\n最后，计算含量的时候，根据吸收系数和吸光度，应用朗伯-比尔定律，A=εcl，其中ε是摩尔吸光系数，但这里可能用的是百分吸收系数（1% 1cm），所以需要确认单位是否正确。例如，吸收系数137可能是指在1%浓度（1g/100ml）下，1cm光程的吸光度，这样计算时需要将浓度转换为相应的单位。\\n\\n可能用户是医生或者药剂师，需要准确的步骤来执行或理解检测流程，所以步骤要清晰，符合药典标准。需要确保步骤中的每个操作都有依据，比如量取的体积、稀释倍数、波长选择等，都符合药典推荐的方法，这样结果才可靠。\\n\\n总结下来，步骤应该是：取样、两次稀释、测定吸光度、计算含量。同时要强调方法的准确性和药典推荐，让用户知道这是标准方法，可信度高。\\n</think> \\n 您好，测定盐酸甲氧明注射液的含量通常采用紫外-可见分光光度法。首先，需要精密量取适量的注射液（大约相当于100毫克盐酸甲氧明），将其置于250毫升的量瓶中，并用水稀释至刻度线，充分摇匀。然后，从这个溶液中精密量取10毫升，放入100毫升的量瓶中，再次用水稀释至刻度线，摇匀。最后，使用紫外-可见分光光度计，在290纳米的波长处测定吸光度，根据C1H1NO3·HCl的吸收系数137来计算盐酸甲氧明的含量。这种方法准确可靠，是药典推荐的测定方法。<|im_end|>\\n'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74c86ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2822c5e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2684b434",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": train_ds[0]['input']},\n",
    "    {\"role\": \"assistant\", \"content\": train_ds[0]['output']},    \n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    "    enable_thinking=True # Switches between thinking and non-thinking modes. Default is True.\n",
    ")\n",
    "text\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e0394a61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|im_start|>system\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\n<|im_start|>user\\n医生，我想了解一下如何测定盐酸甲氧明注射液的含量，具体步骤是什么样的？<|im_end|>\\n<|im_start|>assistant\\n<think>嗯，用户问的是如何测定盐酸甲氧明注射液的含量，具体步骤是什么。首先，我需要回忆一下相关的分析方法，特别是药典中常用的方法。盐酸甲氧明属于药物分析中的常见成分，通常可能用紫外分光光度法或者HPLC之类的。\\n\\n首先，用户提到的是紫外-可见分光光度法，这应该是比较直接的方法。那步骤的话，首先应该考虑样品的制备。因为注射液浓度可能较高，需要稀释到合适的浓度范围，使得吸光度在仪器的线性范围内。比如，先取一定量的注射液，然后稀释到某个体积，比如250毫升的量瓶，这样可能是一个步骤。\\n\\n接下来，可能需要进一步稀释，因为第一次稀释后的溶液可能吸光度还是太高或者太低。比如，从第一次稀释后的溶液中再取一部分，比如10毫升，放到另一个量瓶，比如100毫升，再次稀释。这样两次稀释可以得到合适的浓度。\\n\\n然后，测定吸光度的时候，波长的选择很重要。盐酸甲氧明的最大吸收波长应该查文献或者药典，比如问题中的答案提到的是290纳米，这可能来自药典的规定。需要确认这个波长是否正确，以及吸收系数是否准确。比如，吸收系数137是否对应C1H1NO3·HCl的计算，可能需要根据分子量和浓度来计算，但用户可能不需要详细计算，只需要步骤。\\n\\n另外，步骤中的精密量取需要注意，比如使用移液管或者自动进样器，确保量取的准确性。摇匀也是关键步骤，确保溶液均匀，避免局部浓度不均导致误差。\\n\\n可能还需要考虑空白对照，比如用水代替样品，进行同样的操作，以扣除背景吸光度。不过答案中没有提到，可能在标准方法中已经包含，或者步骤简化了。但作为详细步骤，可能需要补充这一点。\\n\\n最后，计算含量的时候，根据吸收系数和吸光度，应用朗伯-比尔定律，A=εcl，其中ε是摩尔吸光系数，但这里可能用的是百分吸收系数（1% 1cm），所以需要确认单位是否正确。例如，吸收系数137可能是指在1%浓度（1g/100ml）下，1cm光程的吸光度，这样计算时需要将浓度转换为相应的单位。\\n\\n可能用户是医生或者药剂师，需要准确的步骤来执行或理解检测流程，所以步骤要清晰，符合药典标准。需要确保步骤中的每个操作都有依据，比如量取的体积、稀释倍数、波长选择等，都符合药典推荐的方法，这样结果才可靠。\\n\\n总结下来，步骤应该是：取样、两次稀释、测定吸光度、计算含量。同时要强调方法的准确性和药典推荐，让用户知道这是标准方法，可信度高。\\n</think> \\n 您好，测定盐酸甲氧明注射液的含量通常采用紫外-可见分光光度法。首先，需要精密量取适量的注射液（大约相当于100毫克盐酸甲氧明），将其置于250毫升的量瓶中，并用水稀释至刻度线，充分摇匀。然后，从这个溶液中精密量取10毫升，放入100毫升的量瓶中，再次用水稀释至刻度线，摇匀。最后，使用紫外-可见分光光度计，在290纳米的波长处测定吸光度，根据C1H1NO3·HCl的吸收系数137来计算盐酸甲氧明的含量。这种方法准确可靠，是药典推荐的测定方法。<|im_end|>\\n<|im_start|>assistant\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68e1eef",
   "metadata": {},
   "source": [
    "此处的token, 没有用chat_template模板，而是直接手动拼接，　模板函数的功能，也仅仅是把各个字段封装成 start, end的字段\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7148d857",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08494db231dd4c3dba165741fdd17a16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2166 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "train_dataset = train_ds.map(process_func, remove_columns=train_ds.column_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "376e6c99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [151644,\n",
       "  8948,\n",
       "  198,\n",
       "  56568,\n",
       "  101909,\n",
       "  104316,\n",
       "  101057,\n",
       "  3837,\n",
       "  112735,\n",
       "  100345,\n",
       "  20002,\n",
       "  103936,\n",
       "  3837,\n",
       "  107485,\n",
       "  106646,\n",
       "  104107,\n",
       "  111423,\n",
       "  1773,\n",
       "  151645,\n",
       "  198,\n",
       "  151644,\n",
       "  872,\n",
       "  198,\n",
       "  103998,\n",
       "  3837,\n",
       "  104100,\n",
       "  110050,\n",
       "  100007,\n",
       "  115391,\n",
       "  102029,\n",
       "  99918,\n",
       "  100043,\n",
       "  100316,\n",
       "  30858,\n",
       "  107833,\n",
       "  100058,\n",
       "  9370,\n",
       "  104982,\n",
       "  3837,\n",
       "  100398,\n",
       "  105652,\n",
       "  102021,\n",
       "  100535,\n",
       "  11319,\n",
       "  151645,\n",
       "  198,\n",
       "  151644,\n",
       "  77091,\n",
       "  198,\n",
       "  13708,\n",
       "  766,\n",
       "  29,\n",
       "  106287,\n",
       "  3837,\n",
       "  20002,\n",
       "  56007,\n",
       "  100146,\n",
       "  100007,\n",
       "  115391,\n",
       "  102029,\n",
       "  99918,\n",
       "  100043,\n",
       "  100316,\n",
       "  30858,\n",
       "  107833,\n",
       "  100058,\n",
       "  9370,\n",
       "  104982,\n",
       "  3837,\n",
       "  100398,\n",
       "  105652,\n",
       "  102021,\n",
       "  1773,\n",
       "  101140,\n",
       "  3837,\n",
       "  35946,\n",
       "  85106,\n",
       "  104843,\n",
       "  100158,\n",
       "  105470,\n",
       "  101042,\n",
       "  39907,\n",
       "  3837,\n",
       "  104050,\n",
       "  99471,\n",
       "  99548,\n",
       "  15946,\n",
       "  103229,\n",
       "  104339,\n",
       "  1773,\n",
       "  102029,\n",
       "  99918,\n",
       "  100043,\n",
       "  100316,\n",
       "  30858,\n",
       "  100409,\n",
       "  104459,\n",
       "  101042,\n",
       "  101047,\n",
       "  101536,\n",
       "  105024,\n",
       "  3837,\n",
       "  102119,\n",
       "  87267,\n",
       "  11622,\n",
       "  114794,\n",
       "  17177,\n",
       "  99225,\n",
       "  99225,\n",
       "  26381,\n",
       "  24339,\n",
       "  100631,\n",
       "  39,\n",
       "  2916,\n",
       "  34,\n",
       "  106896,\n",
       "  3407,\n",
       "  101140,\n",
       "  3837,\n",
       "  20002,\n",
       "  104496,\n",
       "  100146,\n",
       "  114794,\n",
       "  12,\n",
       "  101479,\n",
       "  17177,\n",
       "  99225,\n",
       "  99225,\n",
       "  26381,\n",
       "  24339,\n",
       "  3837,\n",
       "  43288,\n",
       "  104583,\n",
       "  99792,\n",
       "  101041,\n",
       "  104339,\n",
       "  1773,\n",
       "  99212,\n",
       "  105652,\n",
       "  100363,\n",
       "  3837,\n",
       "  101140,\n",
       "  99730,\n",
       "  101118,\n",
       "  111253,\n",
       "  9370,\n",
       "  43316,\n",
       "  56278,\n",
       "  1773,\n",
       "  99519,\n",
       "  107833,\n",
       "  100058,\n",
       "  108010,\n",
       "  87267,\n",
       "  105540,\n",
       "  3837,\n",
       "  85106,\n",
       "  101474,\n",
       "  68862,\n",
       "  26939,\n",
       "  106873,\n",
       "  108010,\n",
       "  101121,\n",
       "  3837,\n",
       "  104193,\n",
       "  99544,\n",
       "  99225,\n",
       "  26381,\n",
       "  18493,\n",
       "  106550,\n",
       "  9370,\n",
       "  43268,\n",
       "  33071,\n",
       "  104589,\n",
       "  1773,\n",
       "  101912,\n",
       "  3837,\n",
       "  60726,\n",
       "  18158,\n",
       "  99623,\n",
       "  32757,\n",
       "  9370,\n",
       "  107833,\n",
       "  100058,\n",
       "  3837,\n",
       "  101889,\n",
       "  101474,\n",
       "  68862,\n",
       "  26939,\n",
       "  106168,\n",
       "  110238,\n",
       "  3837,\n",
       "  101912,\n",
       "  17,\n",
       "  20,\n",
       "  15,\n",
       "  117033,\n",
       "  9370,\n",
       "  32757,\n",
       "  100822,\n",
       "  3837,\n",
       "  99654,\n",
       "  87267,\n",
       "  101909,\n",
       "  105652,\n",
       "  3407,\n",
       "  104326,\n",
       "  3837,\n",
       "  87267,\n",
       "  85106,\n",
       "  100642,\n",
       "  101474,\n",
       "  68862,\n",
       "  3837,\n",
       "  99519,\n",
       "  104064,\n",
       "  101474,\n",
       "  68862,\n",
       "  104813,\n",
       "  114357,\n",
       "  87267,\n",
       "  99544,\n",
       "  99225,\n",
       "  26381,\n",
       "  99998,\n",
       "  109830,\n",
       "  100631,\n",
       "  99222,\n",
       "  99285,\n",
       "  1773,\n",
       "  101912,\n",
       "  3837,\n",
       "  45181,\n",
       "  104064,\n",
       "  101474,\n",
       "  68862,\n",
       "  104813,\n",
       "  114357,\n",
       "  15946,\n",
       "  87256,\n",
       "  18158,\n",
       "  104954,\n",
       "  3837,\n",
       "  101912,\n",
       "  16,\n",
       "  15,\n",
       "  117033,\n",
       "  3837,\n",
       "  107974,\n",
       "  104887,\n",
       "  32757,\n",
       "  100822,\n",
       "  3837,\n",
       "  101912,\n",
       "  16,\n",
       "  15,\n",
       "  15,\n",
       "  117033,\n",
       "  3837,\n",
       "  103989,\n",
       "  101474,\n",
       "  68862,\n",
       "  1773,\n",
       "  99654,\n",
       "  105177,\n",
       "  101474,\n",
       "  68862,\n",
       "  73670,\n",
       "  101051,\n",
       "  106873,\n",
       "  108010,\n",
       "  3407,\n",
       "  101889,\n",
       "  3837,\n",
       "  115391,\n",
       "  99544,\n",
       "  99225,\n",
       "  26381,\n",
       "  103920,\n",
       "  3837,\n",
       "  99804,\n",
       "  45861,\n",
       "  105340,\n",
       "  106760,\n",
       "  1773,\n",
       "  102029,\n",
       "  99918,\n",
       "  100043,\n",
       "  100316,\n",
       "  30858,\n",
       "  107819,\n",
       "  104460,\n",
       "  99804,\n",
       "  45861,\n",
       "  99730,\n",
       "  32876,\n",
       "  108067,\n",
       "  100631,\n",
       "  99471,\n",
       "  99548,\n",
       "  3837,\n",
       "  101912,\n",
       "  86119,\n",
       "  101047,\n",
       "  102349,\n",
       "  104496,\n",
       "  100146,\n",
       "  17,\n",
       "  24,\n",
       "  15,\n",
       "  111109,\n",
       "  3837,\n",
       "  43288,\n",
       "  87267,\n",
       "  101919,\n",
       "  99471,\n",
       "  99548,\n",
       "  105606,\n",
       "  1773,\n",
       "  85106,\n",
       "  81167,\n",
       "  99487,\n",
       "  99804,\n",
       "  45861,\n",
       "  64471,\n",
       "  88991,\n",
       "  3837,\n",
       "  101034,\n",
       "  104460,\n",
       "  110589,\n",
       "  64471,\n",
       "  102188,\n",
       "  1773,\n",
       "  101912,\n",
       "  3837,\n",
       "  104460,\n",
       "  110589,\n",
       "  16,\n",
       "  18,\n",
       "  22,\n",
       "  64471,\n",
       "  103124,\n",
       "  34,\n",
       "  16,\n",
       "  39,\n",
       "  16,\n",
       "  8996,\n",
       "  18,\n",
       "  13935,\n",
       "  39,\n",
       "  5066,\n",
       "  9370,\n",
       "  100768,\n",
       "  3837,\n",
       "  87267,\n",
       "  85106,\n",
       "  100345,\n",
       "  102388,\n",
       "  32757,\n",
       "  33108,\n",
       "  108010,\n",
       "  36407,\n",
       "  100768,\n",
       "  3837,\n",
       "  77288,\n",
       "  20002,\n",
       "  87267,\n",
       "  104689,\n",
       "  100700,\n",
       "  100768,\n",
       "  3837,\n",
       "  107525,\n",
       "  105652,\n",
       "  3407,\n",
       "  101948,\n",
       "  3837,\n",
       "  105652,\n",
       "  101047,\n",
       "  108740,\n",
       "  32757,\n",
       "  18158,\n",
       "  107916,\n",
       "  3837,\n",
       "  101912,\n",
       "  37029,\n",
       "  59534,\n",
       "  100058,\n",
       "  35551,\n",
       "  100631,\n",
       "  100756,\n",
       "  41299,\n",
       "  90885,\n",
       "  31548,\n",
       "  3837,\n",
       "  103944,\n",
       "  32757,\n",
       "  18158,\n",
       "  9370,\n",
       "  111076,\n",
       "  1773,\n",
       "  100307,\n",
       "  102912,\n",
       "  100000,\n",
       "  99936,\n",
       "  105652,\n",
       "  3837,\n",
       "  103944,\n",
       "  114357,\n",
       "  106674,\n",
       "  3837,\n",
       "  101153,\n",
       "  106304,\n",
       "  108010,\n",
       "  16530,\n",
       "  99472,\n",
       "  100673,\n",
       "  115956,\n",
       "  3407,\n",
       "  87267,\n",
       "  106750,\n",
       "  101118,\n",
       "  107762,\n",
       "  103201,\n",
       "  3837,\n",
       "  101912,\n",
       "  103135,\n",
       "  107456,\n",
       "  111253,\n",
       "  3837,\n",
       "  71817,\n",
       "  107948,\n",
       "  40090,\n",
       "  3837,\n",
       "  23031,\n",
       "  109659,\n",
       "  102193,\n",
       "  99544,\n",
       "  99225,\n",
       "  26381,\n",
       "  1773,\n",
       "  100632,\n",
       "  102349,\n",
       "  15946,\n",
       "  80443,\n",
       "  104496,\n",
       "  3837,\n",
       "  87267,\n",
       "  18493,\n",
       "  100142,\n",
       "  39907,\n",
       "  15946,\n",
       "  99461,\n",
       "  102298,\n",
       "  3837,\n",
       "  100631,\n",
       "  105652,\n",
       "  110487,\n",
       "  34187,\n",
       "  1773,\n",
       "  77288,\n",
       "  100622,\n",
       "  100700,\n",
       "  105652,\n",
       "  3837,\n",
       "  87267,\n",
       "  85106,\n",
       "  104361,\n",
       "  105064,\n",
       "  3407,\n",
       "  100161,\n",
       "  3837,\n",
       "  100768,\n",
       "  104982,\n",
       "  103920,\n",
       "  3837,\n",
       "  100345,\n",
       "  104460,\n",
       "  110589,\n",
       "  33108,\n",
       "  99544,\n",
       "  99225,\n",
       "  26381,\n",
       "  3837,\n",
       "  99892,\n",
       "  100106,\n",
       "  102044,\n",
       "  12,\n",
       "  56006,\n",
       "  99079,\n",
       "  117250,\n",
       "  3837,\n",
       "  32,\n",
       "  28,\n",
       "  30143,\n",
       "  564,\n",
       "  3837,\n",
       "  90919,\n",
       "  30143,\n",
       "  20412,\n",
       "  100487,\n",
       "  99079,\n",
       "  99544,\n",
       "  99225,\n",
       "  110589,\n",
       "  3837,\n",
       "  77288,\n",
       "  99817,\n",
       "  87267,\n",
       "  11622,\n",
       "  100146,\n",
       "  101630,\n",
       "  104460,\n",
       "  110589,\n",
       "  9909,\n",
       "  16,\n",
       "  4,\n",
       "  220,\n",
       "  16,\n",
       "  6226,\n",
       "  48272,\n",
       "  99999,\n",
       "  85106,\n",
       "  81167,\n",
       "  75317,\n",
       "  64471,\n",
       "  88991,\n",
       "  1773,\n",
       "  77557,\n",
       "  3837,\n",
       "  104460,\n",
       "  110589,\n",
       "  16,\n",
       "  18,\n",
       "  22,\n",
       "  87267,\n",
       "  104442,\n",
       "  18493,\n",
       "  16,\n",
       "  4,\n",
       "  108010,\n",
       "  9909,\n",
       "  16,\n",
       "  70,\n",
       "  14,\n",
       "  16,\n",
       "  15,\n",
       "  15,\n",
       "  1014,\n",
       "  7552,\n",
       "  16872,\n",
       "  3837,\n",
       "  16,\n",
       "  6226,\n",
       "  99225,\n",
       "  38507,\n",
       "  9370,\n",
       "  99544,\n",
       "  99225,\n",
       "  26381,\n",
       "  3837,\n",
       "  99654,\n",
       "  100768,\n",
       "  13343,\n",
       "  85106,\n",
       "  44063,\n",
       "  108010,\n",
       "  105359,\n",
       "  17714,\n",
       "  105004,\n",
       "  75317,\n",
       "  3407,\n",
       "  87267,\n",
       "  20002,\n",
       "  20412,\n",
       "  103998,\n",
       "  100631,\n",
       "  99471,\n",
       "  100067,\n",
       "  99235,\n",
       "  3837,\n",
       "  85106,\n",
       "  102188,\n",
       "  9370,\n",
       "  105652,\n",
       "  36407,\n",
       "  75117,\n",
       "  57191,\n",
       "  101128,\n",
       "  101978,\n",
       "  102054,\n",
       "  3837,\n",
       "  99999,\n",
       "  105652,\n",
       "  30534,\n",
       "  104542,\n",
       "  3837,\n",
       "  101137,\n",
       "  99471,\n",
       "  99548,\n",
       "  100142,\n",
       "  1773,\n",
       "  85106,\n",
       "  103944,\n",
       "  105652,\n",
       "  101047,\n",
       "  103991,\n",
       "  40090,\n",
       "  101103,\n",
       "  104282,\n",
       "  3837,\n",
       "  101912,\n",
       "  32757,\n",
       "  18158,\n",
       "  9370,\n",
       "  110238,\n",
       "  5373,\n",
       "  101474,\n",
       "  68862,\n",
       "  97306,\n",
       "  8863,\n",
       "  5373,\n",
       "  99804,\n",
       "  45861,\n",
       "  50404,\n",
       "  49567,\n",
       "  3837,\n",
       "  71268,\n",
       "  101137,\n",
       "  99471,\n",
       "  99548,\n",
       "  101914,\n",
       "  104339,\n",
       "  3837,\n",
       "  99654,\n",
       "  59151,\n",
       "  99306,\n",
       "  101650,\n",
       "  3407,\n",
       "  102050,\n",
       "  100194,\n",
       "  3837,\n",
       "  105652,\n",
       "  104583,\n",
       "  5122,\n",
       "  18158,\n",
       "  90885,\n",
       "  5373,\n",
       "  105177,\n",
       "  101474,\n",
       "  68862,\n",
       "  5373,\n",
       "  115391,\n",
       "  99544,\n",
       "  99225,\n",
       "  26381,\n",
       "  5373,\n",
       "  100768,\n",
       "  104982,\n",
       "  1773,\n",
       "  91572,\n",
       "  30534,\n",
       "  104046,\n",
       "  39907,\n",
       "  9370,\n",
       "  102188,\n",
       "  105178,\n",
       "  99471,\n",
       "  99548,\n",
       "  101914,\n",
       "  3837,\n",
       "  115987,\n",
       "  99392,\n",
       "  100346,\n",
       "  100142,\n",
       "  39907,\n",
       "  3837,\n",
       "  113546,\n",
       "  26381,\n",
       "  44636,\n",
       "  8997,\n",
       "  522,\n",
       "  26865,\n",
       "  29,\n",
       "  715,\n",
       "  6567,\n",
       "  60757,\n",
       "  52801,\n",
       "  3837,\n",
       "  115391,\n",
       "  102029,\n",
       "  99918,\n",
       "  100043,\n",
       "  100316,\n",
       "  30858,\n",
       "  107833,\n",
       "  100058,\n",
       "  9370,\n",
       "  104982,\n",
       "  102119,\n",
       "  101910,\n",
       "  114794,\n",
       "  12,\n",
       "  101479,\n",
       "  17177,\n",
       "  99225,\n",
       "  99225,\n",
       "  26381,\n",
       "  24339,\n",
       "  1773,\n",
       "  101140,\n",
       "  3837,\n",
       "  85106,\n",
       "  108740,\n",
       "  32757,\n",
       "  18158,\n",
       "  106968,\n",
       "  9370,\n",
       "  107833,\n",
       "  100058,\n",
       "  9909,\n",
       "  104995,\n",
       "  106153,\n",
       "  16,\n",
       "  15,\n",
       "  15,\n",
       "  118930,\n",
       "  102029,\n",
       "  99918,\n",
       "  100043,\n",
       "  100316,\n",
       "  30858,\n",
       "  48272,\n",
       "  105278,\n",
       "  113569,\n",
       "  17,\n",
       "  20,\n",
       "  15,\n",
       "  117033,\n",
       "  9370,\n",
       "  32757,\n",
       "  100822,\n",
       "  15946,\n",
       "  90395,\n",
       "  103135,\n",
       "  101474,\n",
       "  68862,\n",
       "  56137,\n",
       "  99483,\n",
       "  26381,\n",
       "  43268,\n",
       "  3837,\n",
       "  100418,\n",
       "  100307,\n",
       "  102912,\n",
       "  1773,\n",
       "  101889,\n",
       "  3837,\n",
       "  45181,\n",
       "  99487,\n",
       "  114357,\n",
       "  15946,\n",
       "  108740,\n",
       "  32757,\n",
       "  18158,\n",
       "  16,\n",
       "  15,\n",
       "  117033,\n",
       "  3837,\n",
       "  105851,\n",
       "  16,\n",
       "  15,\n",
       "  15,\n",
       "  117033,\n",
       "  9370,\n",
       "  32757,\n",
       "  100822,\n",
       "  15946,\n",
       "  3837,\n",
       "  103989,\n",
       "  103135,\n",
       "  101474,\n",
       "  68862,\n",
       "  56137,\n",
       "  99483,\n",
       "  26381,\n",
       "  43268,\n",
       "  3837,\n",
       "  100307,\n",
       "  102912,\n",
       "  1773,\n",
       "  100161,\n",
       "  3837,\n",
       "  37029,\n",
       "  114794,\n",
       "  12,\n",
       "  101479,\n",
       "  17177,\n",
       "  99225,\n",
       "  99225,\n",
       "  26381,\n",
       "  37643,\n",
       "  96050,\n",
       "  17,\n",
       "  24,\n",
       "  15,\n",
       "  111109,\n",
       "  9370,\n",
       "  99804,\n",
       "  45861,\n",
       "  44290,\n",
       "  115391,\n",
       "  99544,\n",
       "  99225,\n",
       "  26381,\n",
       "  3837,\n",
       "  100345,\n",
       "  34,\n",
       "  16,\n",
       "  39,\n",
       "  16,\n",
       "  8996,\n",
       "  18,\n",
       "  13935,\n",
       "  39,\n",
       "  5066,\n",
       "  9370,\n",
       "  104460,\n",
       "  110589,\n",
       "  16,\n",
       "  18,\n",
       "  22,\n",
       "  36407,\n",
       "  100768,\n",
       "  102029,\n",
       "  99918,\n",
       "  100043,\n",
       "  100316,\n",
       "  30858,\n",
       "  9370,\n",
       "  104982,\n",
       "  1773,\n",
       "  115195,\n",
       "  102188,\n",
       "  101650,\n",
       "  3837,\n",
       "  20412,\n",
       "  99471,\n",
       "  99548,\n",
       "  101914,\n",
       "  9370,\n",
       "  115391,\n",
       "  39907,\n",
       "  1773,\n",
       "  151645],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " 'labels': [-100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  13708,\n",
       "  766,\n",
       "  29,\n",
       "  106287,\n",
       "  3837,\n",
       "  20002,\n",
       "  56007,\n",
       "  100146,\n",
       "  100007,\n",
       "  115391,\n",
       "  102029,\n",
       "  99918,\n",
       "  100043,\n",
       "  100316,\n",
       "  30858,\n",
       "  107833,\n",
       "  100058,\n",
       "  9370,\n",
       "  104982,\n",
       "  3837,\n",
       "  100398,\n",
       "  105652,\n",
       "  102021,\n",
       "  1773,\n",
       "  101140,\n",
       "  3837,\n",
       "  35946,\n",
       "  85106,\n",
       "  104843,\n",
       "  100158,\n",
       "  105470,\n",
       "  101042,\n",
       "  39907,\n",
       "  3837,\n",
       "  104050,\n",
       "  99471,\n",
       "  99548,\n",
       "  15946,\n",
       "  103229,\n",
       "  104339,\n",
       "  1773,\n",
       "  102029,\n",
       "  99918,\n",
       "  100043,\n",
       "  100316,\n",
       "  30858,\n",
       "  100409,\n",
       "  104459,\n",
       "  101042,\n",
       "  101047,\n",
       "  101536,\n",
       "  105024,\n",
       "  3837,\n",
       "  102119,\n",
       "  87267,\n",
       "  11622,\n",
       "  114794,\n",
       "  17177,\n",
       "  99225,\n",
       "  99225,\n",
       "  26381,\n",
       "  24339,\n",
       "  100631,\n",
       "  39,\n",
       "  2916,\n",
       "  34,\n",
       "  106896,\n",
       "  3407,\n",
       "  101140,\n",
       "  3837,\n",
       "  20002,\n",
       "  104496,\n",
       "  100146,\n",
       "  114794,\n",
       "  12,\n",
       "  101479,\n",
       "  17177,\n",
       "  99225,\n",
       "  99225,\n",
       "  26381,\n",
       "  24339,\n",
       "  3837,\n",
       "  43288,\n",
       "  104583,\n",
       "  99792,\n",
       "  101041,\n",
       "  104339,\n",
       "  1773,\n",
       "  99212,\n",
       "  105652,\n",
       "  100363,\n",
       "  3837,\n",
       "  101140,\n",
       "  99730,\n",
       "  101118,\n",
       "  111253,\n",
       "  9370,\n",
       "  43316,\n",
       "  56278,\n",
       "  1773,\n",
       "  99519,\n",
       "  107833,\n",
       "  100058,\n",
       "  108010,\n",
       "  87267,\n",
       "  105540,\n",
       "  3837,\n",
       "  85106,\n",
       "  101474,\n",
       "  68862,\n",
       "  26939,\n",
       "  106873,\n",
       "  108010,\n",
       "  101121,\n",
       "  3837,\n",
       "  104193,\n",
       "  99544,\n",
       "  99225,\n",
       "  26381,\n",
       "  18493,\n",
       "  106550,\n",
       "  9370,\n",
       "  43268,\n",
       "  33071,\n",
       "  104589,\n",
       "  1773,\n",
       "  101912,\n",
       "  3837,\n",
       "  60726,\n",
       "  18158,\n",
       "  99623,\n",
       "  32757,\n",
       "  9370,\n",
       "  107833,\n",
       "  100058,\n",
       "  3837,\n",
       "  101889,\n",
       "  101474,\n",
       "  68862,\n",
       "  26939,\n",
       "  106168,\n",
       "  110238,\n",
       "  3837,\n",
       "  101912,\n",
       "  17,\n",
       "  20,\n",
       "  15,\n",
       "  117033,\n",
       "  9370,\n",
       "  32757,\n",
       "  100822,\n",
       "  3837,\n",
       "  99654,\n",
       "  87267,\n",
       "  101909,\n",
       "  105652,\n",
       "  3407,\n",
       "  104326,\n",
       "  3837,\n",
       "  87267,\n",
       "  85106,\n",
       "  100642,\n",
       "  101474,\n",
       "  68862,\n",
       "  3837,\n",
       "  99519,\n",
       "  104064,\n",
       "  101474,\n",
       "  68862,\n",
       "  104813,\n",
       "  114357,\n",
       "  87267,\n",
       "  99544,\n",
       "  99225,\n",
       "  26381,\n",
       "  99998,\n",
       "  109830,\n",
       "  100631,\n",
       "  99222,\n",
       "  99285,\n",
       "  1773,\n",
       "  101912,\n",
       "  3837,\n",
       "  45181,\n",
       "  104064,\n",
       "  101474,\n",
       "  68862,\n",
       "  104813,\n",
       "  114357,\n",
       "  15946,\n",
       "  87256,\n",
       "  18158,\n",
       "  104954,\n",
       "  3837,\n",
       "  101912,\n",
       "  16,\n",
       "  15,\n",
       "  117033,\n",
       "  3837,\n",
       "  107974,\n",
       "  104887,\n",
       "  32757,\n",
       "  100822,\n",
       "  3837,\n",
       "  101912,\n",
       "  16,\n",
       "  15,\n",
       "  15,\n",
       "  117033,\n",
       "  3837,\n",
       "  103989,\n",
       "  101474,\n",
       "  68862,\n",
       "  1773,\n",
       "  99654,\n",
       "  105177,\n",
       "  101474,\n",
       "  68862,\n",
       "  73670,\n",
       "  101051,\n",
       "  106873,\n",
       "  108010,\n",
       "  3407,\n",
       "  101889,\n",
       "  3837,\n",
       "  115391,\n",
       "  99544,\n",
       "  99225,\n",
       "  26381,\n",
       "  103920,\n",
       "  3837,\n",
       "  99804,\n",
       "  45861,\n",
       "  105340,\n",
       "  106760,\n",
       "  1773,\n",
       "  102029,\n",
       "  99918,\n",
       "  100043,\n",
       "  100316,\n",
       "  30858,\n",
       "  107819,\n",
       "  104460,\n",
       "  99804,\n",
       "  45861,\n",
       "  99730,\n",
       "  32876,\n",
       "  108067,\n",
       "  100631,\n",
       "  99471,\n",
       "  99548,\n",
       "  3837,\n",
       "  101912,\n",
       "  86119,\n",
       "  101047,\n",
       "  102349,\n",
       "  104496,\n",
       "  100146,\n",
       "  17,\n",
       "  24,\n",
       "  15,\n",
       "  111109,\n",
       "  3837,\n",
       "  43288,\n",
       "  87267,\n",
       "  101919,\n",
       "  99471,\n",
       "  99548,\n",
       "  105606,\n",
       "  1773,\n",
       "  85106,\n",
       "  81167,\n",
       "  99487,\n",
       "  99804,\n",
       "  45861,\n",
       "  64471,\n",
       "  88991,\n",
       "  3837,\n",
       "  101034,\n",
       "  104460,\n",
       "  110589,\n",
       "  64471,\n",
       "  102188,\n",
       "  1773,\n",
       "  101912,\n",
       "  3837,\n",
       "  104460,\n",
       "  110589,\n",
       "  16,\n",
       "  18,\n",
       "  22,\n",
       "  64471,\n",
       "  103124,\n",
       "  34,\n",
       "  16,\n",
       "  39,\n",
       "  16,\n",
       "  8996,\n",
       "  18,\n",
       "  13935,\n",
       "  39,\n",
       "  5066,\n",
       "  9370,\n",
       "  100768,\n",
       "  3837,\n",
       "  87267,\n",
       "  85106,\n",
       "  100345,\n",
       "  102388,\n",
       "  32757,\n",
       "  33108,\n",
       "  108010,\n",
       "  36407,\n",
       "  100768,\n",
       "  3837,\n",
       "  77288,\n",
       "  20002,\n",
       "  87267,\n",
       "  104689,\n",
       "  100700,\n",
       "  100768,\n",
       "  3837,\n",
       "  107525,\n",
       "  105652,\n",
       "  3407,\n",
       "  101948,\n",
       "  3837,\n",
       "  105652,\n",
       "  101047,\n",
       "  108740,\n",
       "  32757,\n",
       "  18158,\n",
       "  107916,\n",
       "  3837,\n",
       "  101912,\n",
       "  37029,\n",
       "  59534,\n",
       "  100058,\n",
       "  35551,\n",
       "  100631,\n",
       "  100756,\n",
       "  41299,\n",
       "  90885,\n",
       "  31548,\n",
       "  3837,\n",
       "  103944,\n",
       "  32757,\n",
       "  18158,\n",
       "  9370,\n",
       "  111076,\n",
       "  1773,\n",
       "  100307,\n",
       "  102912,\n",
       "  100000,\n",
       "  99936,\n",
       "  105652,\n",
       "  3837,\n",
       "  103944,\n",
       "  114357,\n",
       "  106674,\n",
       "  3837,\n",
       "  101153,\n",
       "  106304,\n",
       "  108010,\n",
       "  16530,\n",
       "  99472,\n",
       "  100673,\n",
       "  115956,\n",
       "  3407,\n",
       "  87267,\n",
       "  106750,\n",
       "  101118,\n",
       "  107762,\n",
       "  103201,\n",
       "  3837,\n",
       "  101912,\n",
       "  103135,\n",
       "  107456,\n",
       "  111253,\n",
       "  3837,\n",
       "  71817,\n",
       "  107948,\n",
       "  40090,\n",
       "  3837,\n",
       "  23031,\n",
       "  109659,\n",
       "  102193,\n",
       "  99544,\n",
       "  99225,\n",
       "  26381,\n",
       "  1773,\n",
       "  100632,\n",
       "  102349,\n",
       "  15946,\n",
       "  80443,\n",
       "  104496,\n",
       "  3837,\n",
       "  87267,\n",
       "  18493,\n",
       "  100142,\n",
       "  39907,\n",
       "  15946,\n",
       "  99461,\n",
       "  102298,\n",
       "  3837,\n",
       "  100631,\n",
       "  105652,\n",
       "  110487,\n",
       "  34187,\n",
       "  1773,\n",
       "  77288,\n",
       "  100622,\n",
       "  100700,\n",
       "  105652,\n",
       "  3837,\n",
       "  87267,\n",
       "  85106,\n",
       "  104361,\n",
       "  105064,\n",
       "  3407,\n",
       "  100161,\n",
       "  3837,\n",
       "  100768,\n",
       "  104982,\n",
       "  103920,\n",
       "  3837,\n",
       "  100345,\n",
       "  104460,\n",
       "  110589,\n",
       "  33108,\n",
       "  99544,\n",
       "  99225,\n",
       "  26381,\n",
       "  3837,\n",
       "  99892,\n",
       "  100106,\n",
       "  102044,\n",
       "  12,\n",
       "  56006,\n",
       "  99079,\n",
       "  117250,\n",
       "  3837,\n",
       "  32,\n",
       "  28,\n",
       "  30143,\n",
       "  564,\n",
       "  3837,\n",
       "  90919,\n",
       "  30143,\n",
       "  20412,\n",
       "  100487,\n",
       "  99079,\n",
       "  99544,\n",
       "  99225,\n",
       "  110589,\n",
       "  3837,\n",
       "  77288,\n",
       "  99817,\n",
       "  87267,\n",
       "  11622,\n",
       "  100146,\n",
       "  101630,\n",
       "  104460,\n",
       "  110589,\n",
       "  9909,\n",
       "  16,\n",
       "  4,\n",
       "  220,\n",
       "  16,\n",
       "  6226,\n",
       "  48272,\n",
       "  99999,\n",
       "  85106,\n",
       "  81167,\n",
       "  75317,\n",
       "  64471,\n",
       "  88991,\n",
       "  1773,\n",
       "  77557,\n",
       "  3837,\n",
       "  104460,\n",
       "  110589,\n",
       "  16,\n",
       "  18,\n",
       "  22,\n",
       "  87267,\n",
       "  104442,\n",
       "  18493,\n",
       "  16,\n",
       "  4,\n",
       "  108010,\n",
       "  9909,\n",
       "  16,\n",
       "  70,\n",
       "  14,\n",
       "  16,\n",
       "  15,\n",
       "  15,\n",
       "  1014,\n",
       "  7552,\n",
       "  16872,\n",
       "  3837,\n",
       "  16,\n",
       "  6226,\n",
       "  99225,\n",
       "  38507,\n",
       "  9370,\n",
       "  99544,\n",
       "  99225,\n",
       "  26381,\n",
       "  3837,\n",
       "  99654,\n",
       "  100768,\n",
       "  13343,\n",
       "  85106,\n",
       "  44063,\n",
       "  108010,\n",
       "  105359,\n",
       "  17714,\n",
       "  105004,\n",
       "  75317,\n",
       "  3407,\n",
       "  87267,\n",
       "  20002,\n",
       "  20412,\n",
       "  103998,\n",
       "  100631,\n",
       "  99471,\n",
       "  100067,\n",
       "  99235,\n",
       "  3837,\n",
       "  85106,\n",
       "  102188,\n",
       "  9370,\n",
       "  105652,\n",
       "  36407,\n",
       "  75117,\n",
       "  57191,\n",
       "  101128,\n",
       "  101978,\n",
       "  102054,\n",
       "  3837,\n",
       "  99999,\n",
       "  105652,\n",
       "  30534,\n",
       "  104542,\n",
       "  3837,\n",
       "  101137,\n",
       "  99471,\n",
       "  99548,\n",
       "  100142,\n",
       "  1773,\n",
       "  85106,\n",
       "  103944,\n",
       "  105652,\n",
       "  101047,\n",
       "  103991,\n",
       "  40090,\n",
       "  101103,\n",
       "  104282,\n",
       "  3837,\n",
       "  101912,\n",
       "  32757,\n",
       "  18158,\n",
       "  9370,\n",
       "  110238,\n",
       "  5373,\n",
       "  101474,\n",
       "  68862,\n",
       "  97306,\n",
       "  8863,\n",
       "  5373,\n",
       "  99804,\n",
       "  45861,\n",
       "  50404,\n",
       "  49567,\n",
       "  3837,\n",
       "  71268,\n",
       "  101137,\n",
       "  99471,\n",
       "  99548,\n",
       "  101914,\n",
       "  104339,\n",
       "  3837,\n",
       "  99654,\n",
       "  59151,\n",
       "  99306,\n",
       "  101650,\n",
       "  3407,\n",
       "  102050,\n",
       "  100194,\n",
       "  3837,\n",
       "  105652,\n",
       "  104583,\n",
       "  5122,\n",
       "  18158,\n",
       "  90885,\n",
       "  5373,\n",
       "  105177,\n",
       "  101474,\n",
       "  68862,\n",
       "  5373,\n",
       "  115391,\n",
       "  99544,\n",
       "  99225,\n",
       "  26381,\n",
       "  5373,\n",
       "  100768,\n",
       "  104982,\n",
       "  1773,\n",
       "  91572,\n",
       "  30534,\n",
       "  104046,\n",
       "  39907,\n",
       "  9370,\n",
       "  102188,\n",
       "  105178,\n",
       "  99471,\n",
       "  99548,\n",
       "  101914,\n",
       "  3837,\n",
       "  115987,\n",
       "  99392,\n",
       "  100346,\n",
       "  100142,\n",
       "  39907,\n",
       "  3837,\n",
       "  113546,\n",
       "  26381,\n",
       "  44636,\n",
       "  8997,\n",
       "  522,\n",
       "  26865,\n",
       "  29,\n",
       "  715,\n",
       "  6567,\n",
       "  60757,\n",
       "  52801,\n",
       "  3837,\n",
       "  115391,\n",
       "  102029,\n",
       "  99918,\n",
       "  100043,\n",
       "  100316,\n",
       "  30858,\n",
       "  107833,\n",
       "  100058,\n",
       "  9370,\n",
       "  104982,\n",
       "  102119,\n",
       "  101910,\n",
       "  114794,\n",
       "  12,\n",
       "  101479,\n",
       "  17177,\n",
       "  99225,\n",
       "  99225,\n",
       "  26381,\n",
       "  24339,\n",
       "  1773,\n",
       "  101140,\n",
       "  3837,\n",
       "  85106,\n",
       "  108740,\n",
       "  32757,\n",
       "  18158,\n",
       "  106968,\n",
       "  9370,\n",
       "  107833,\n",
       "  100058,\n",
       "  9909,\n",
       "  104995,\n",
       "  106153,\n",
       "  16,\n",
       "  15,\n",
       "  15,\n",
       "  118930,\n",
       "  102029,\n",
       "  99918,\n",
       "  100043,\n",
       "  100316,\n",
       "  30858,\n",
       "  48272,\n",
       "  105278,\n",
       "  113569,\n",
       "  17,\n",
       "  20,\n",
       "  15,\n",
       "  117033,\n",
       "  9370,\n",
       "  32757,\n",
       "  100822,\n",
       "  15946,\n",
       "  90395,\n",
       "  103135,\n",
       "  101474,\n",
       "  68862,\n",
       "  56137,\n",
       "  99483,\n",
       "  26381,\n",
       "  43268,\n",
       "  3837,\n",
       "  100418,\n",
       "  100307,\n",
       "  102912,\n",
       "  1773,\n",
       "  101889,\n",
       "  3837,\n",
       "  45181,\n",
       "  99487,\n",
       "  114357,\n",
       "  15946,\n",
       "  108740,\n",
       "  32757,\n",
       "  18158,\n",
       "  16,\n",
       "  15,\n",
       "  117033,\n",
       "  3837,\n",
       "  105851,\n",
       "  16,\n",
       "  15,\n",
       "  15,\n",
       "  117033,\n",
       "  9370,\n",
       "  32757,\n",
       "  100822,\n",
       "  15946,\n",
       "  3837,\n",
       "  103989,\n",
       "  103135,\n",
       "  101474,\n",
       "  68862,\n",
       "  56137,\n",
       "  99483,\n",
       "  26381,\n",
       "  43268,\n",
       "  3837,\n",
       "  100307,\n",
       "  102912,\n",
       "  1773,\n",
       "  100161,\n",
       "  3837,\n",
       "  37029,\n",
       "  114794,\n",
       "  12,\n",
       "  101479,\n",
       "  17177,\n",
       "  99225,\n",
       "  99225,\n",
       "  26381,\n",
       "  37643,\n",
       "  96050,\n",
       "  17,\n",
       "  24,\n",
       "  15,\n",
       "  111109,\n",
       "  9370,\n",
       "  99804,\n",
       "  45861,\n",
       "  44290,\n",
       "  115391,\n",
       "  99544,\n",
       "  99225,\n",
       "  26381,\n",
       "  3837,\n",
       "  100345,\n",
       "  34,\n",
       "  16,\n",
       "  39,\n",
       "  16,\n",
       "  8996,\n",
       "  18,\n",
       "  13935,\n",
       "  39,\n",
       "  5066,\n",
       "  9370,\n",
       "  104460,\n",
       "  110589,\n",
       "  16,\n",
       "  18,\n",
       "  22,\n",
       "  36407,\n",
       "  100768,\n",
       "  102029,\n",
       "  99918,\n",
       "  100043,\n",
       "  100316,\n",
       "  30858,\n",
       "  9370,\n",
       "  104982,\n",
       "  1773,\n",
       "  115195,\n",
       "  102188,\n",
       "  101650,\n",
       "  3837,\n",
       "  20412,\n",
       "  99471,\n",
       "  99548,\n",
       "  101914,\n",
       "  9370,\n",
       "  115391,\n",
       "  39907,\n",
       "  1773,\n",
       "  151645]}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6d62e5e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "235565fe6f144ccfa3d8cb320ef68def",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/241 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# 得到验证集\n",
    "eval_df = pd.read_json(test_jsonl_new_path, lines=True)\n",
    "eval_ds = Dataset.from_pandas(eval_df)\n",
    "eval_dataset = eval_ds.map(process_func, remove_columns=eval_ds.column_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d422bfdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "933"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset[2]['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e51f93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "56da4e9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6a0b05e3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Question: 1895年德国物理学教授伦琴的发现对医学影像学的发展有何具体影响？请从技术进步、学科建立和临床应用三个方面进行分析。\n",
      "\n",
      "    LLM:<think>嗯，用户问的是1895年德国物理学家伦琴如何影响医学影像学，以及具体的技术进步、学科建立和临床应用方面。首先，我需要回忆一下伦琴的相关知识。伦琴是X线的发明者之一，对吧？不过他可能不太直接参与实际应用。\n",
      "\n",
      "首先，技术进步方面，我记得之前没提到过，但可能在后期会有所发展。比如CT、MR之类的现代设备可能通过他改进或开发。另外，放射科的设施可能增加，比如更先进的X光机或者扫描仪，这对诊断来说很重要。\n",
      "\n",
      "然后是学科建立。因为X线确实是基础，所以后续学科如放射科应该成立。另外，图像重建技术可能来自他的研究，比如像计算机断层扫描（CT）或者螺旋CT，这些现在很常用。这可能是因为他提出了直线聚焦理论，后来被应用于成像。\n",
      "\n",
      "接下来临床应用部分，可能涉及医学影像的应用场景，比如医生用X光检查患者，或者做PET检测癌症。不过具体的例子可能需要查证，比如CT血管成像（CTA）、MRI这些。另外，可能还有其他领域的应用，比如超声或CT/MR融合，但用户的问题主要集中在医学影像上。\n",
      "\n",
      "还要注意术语是否正确。比如“放射科”是不是正确的？可能用户混淆了不同的名称。另外，是否有其他术语需要解释？\n",
      "\n",
      "另外，有没有可能遗漏的关键点呢？比如，伦琴发现了X线，而不仅仅是医疗用途？可能他更关注X光本身，而不是医学影像。另外，技术上的突破，比如CT、MR等，可能也是其贡献的一部分。\n",
      "\n",
      "可能还需要考虑其他因素，比如他可能的研究方向变化，比如转向非接触性检查，或者与其他科学家合作，推动医学影像的进步。不过根据问题，重点放在医学应用。\n",
      "\n",
      "总结下来，思考过程应该是先确认技术进步，再分解到各个领域，并详细说明每个领域的影响，最后联系到医学中的应用。同时要确保逻辑连贯，没有错误。\n",
      "</think> \n",
      " 当然可以。1895年，德国物理学家伦琴发现了X线。这一发现为医学影像学提供了重要的基础。随着医学影像技术的不断进步，它已经渗透到了医学诊断中。例如，CT、MR等现代医学影像设备都是由他设计和研发的。此外，图像重建技术也可能源于他的工作。这些技术不仅提高了诊断的准确性，也为治疗提供了更多的可能性。希望这些信息对您的研究有所帮助。\n",
      "    \n",
      "\n",
      "    Question: 医生，我最近胃部不适，听说有几种抗溃疡药物可以治疗，您能详细介绍一下这些药物的分类、作用机制以及它们是如何影响胃黏膜的保护与损伤平衡的吗？\n",
      "\n",
      "    LLM:<think>嗯，用户问的是关于三种抗溃疡药的分类和作用机制，以及它们如何影响胃黏膜的保护与损伤平衡。首先，我需要回忆一下抗溃疡药的基本知识。抗溃疡药通常用于胃食管反流病（GERD）或上消化道出血，对吧？\n",
      "\n",
      "那第一种是质子泵抑制剂（PPIs）。这类药物包括奥美拉唑、兰索拉唑等，主要通过阻断H+钾通道来减少胃酸分泌。然后是CYP3A4抑制剂如伏立康唑，可能通过竞争性地结合肝脏中的CYP酶来起效。\n",
      "\n",
      "接下来要考虑它们的作用机制。比如，PPIs可能直接抑制H+/K+-ATP复合物，而CYP3A4抑制剂可能竞争性地阻止这个复合物的形成。另外，它们可能影响胃黏液的合成或者分泌，比如黏液屏障受损后导致炎症反应。\n",
      "\n",
      "然后要解释它们如何影响胃黏膜的保护和损伤平衡。比如，如果PPIs直接杀死胃黏细胞，可能会破坏正常的屏障，让细菌进入，进而引发感染。而CYP3A4抑制剂可能干扰代谢，或者在胃内浓度过高时被吸收，导致黏液不足或氧化应激。\n",
      "\n",
      "不过，用户的问题里没有提到副作用，所以重点放在药物本身的作用机制。另外，他们可能想知道为什么选择特定的药物而不是其他。比如，PPIs针对的是胃酸过多，而CYP3A4抑制剂针对的是肝药酶，这可能是因为它们结构不同，但功能相似。\n",
      "\n",
      "还要考虑是否有其他类型，比如H2受体拮抗剂（H2Is），或者非甾体抗炎药（NSAIDs）。不过用户的问题中没提到，可能不需要。\n",
      "\n",
      "总结下来，回答的结构应该是先列出三种药，每个说明分类和作用机制，再分别分析其对胃黏膜的影响。同时要强调剂量和注意事项，比如停药期和监测。\n",
      "</think> \n",
      " 当然可以。根据您的描述，三类抗溃疡药分别是质子泵抑制剂（PPIs）、CYP3A4抑制剂和H2受体拮抗剂。其中，PPIs属于抑制H+钾-ATPase的药物，它们能够有效地减少胃内的pH值，从而降低胃酸分泌。CYP3A4抑制剂例如伏立康唑，这类药物通过竞争性地结合肝脏中的CYP酶来起效。H2受体拮抗剂则主要用于治疗胃食管反流病，这类药物能够有效减少胃酸分泌。此外，还有其他类型的抗溃疡药，比如铋剂、硫糖铝等，但根据问题，重点还是在三种药物上。每种药物都有其适应症和禁忌症，因此在使用前请务必咨询医生。\n",
      "    \n",
      "\n",
      "    Question: 我最近被诊断出患有淋巴瘤，医生提到这可能导致发热。请问这是由于淋巴瘤组织的坏死和细胞破坏引起的吗？如果是，具体机制是什么？\n",
      "\n",
      "    LLM:<think>嗯，用户问的是淋巴瘤导致发热的原因以及具体机制。首先，我需要回忆一下淋巴瘤的基本知识。淋巴瘤是免疫系统攻击正常组织的结果，比如淋巴结、骨髓或者其他器官。\n",
      "\n",
      "发热可能有多种原因，但主要还是感染或炎症反应。比如细菌性感染，比如呼吸道感染、尿路感染等。那淋巴瘤相关的发热应该和免疫抑制有关？\n",
      "\n",
      "接下来，机制方面，淋巴瘤患者通常会有全身性的免疫抑制，比如使用免疫抑制剂（如环孢素）或者化疗。这时候免疫系统会攻击身体的正常组织，导致炎症反应。比如，肿瘤细胞释放的细胞因子或炎症介质，比如TNF-α、IL-6这些分子。\n",
      "\n",
      "然后，可能还需要考虑其他因素，比如病毒感染（比如流感病毒），或者自身免疫性疾病。不过用户的问题里没有提到病毒，所以重点在细菌和免疫抑制。\n",
      "\n",
      "另外，用户可能担心感染的严重程度，或者是否伴随其他症状。比如头痛、肌肉疼痛、体重下降，这些也可能提示感染。同时，持续发热可能提示恶性进展或更严重的并发症。\n",
      "\n",
      "还要注意，用户可能混淆了普通感染与恶性肿瘤的关系，特别是如果存在免疫抑制的情况下。比如，长期使用免疫抑制剂的患者更容易出现感染，而恶性肿瘤本身可能不直接引起发热，而是因为免疫抑制导致的全身炎症反应。\n",
      "\n",
      "可能还要提到感染后的反应，比如体温升高，皮肤红肿、溃疡。此外，免疫抑制的影响还可能通过血液检查显示白细胞减少、血栓形成，这些变化对发热也有帮助。\n",
      "\n",
      "最后，综合起来，机制应该是免疫抑制 + 病原体感染 + 免疫系统反应。可能还有其他因素，比如慢性炎症状态，但用户的问题中没有提及，所以重点在感染。\n",
      "\n",
      "总结下来，答案中的机制包括：免疫抑制、病原体感染、免疫系统反应。同时要强调及时就医的重要性，因为感染有时需要抗生素治疗。\n",
      "</think> \n",
      " 是的，淋巴瘤确实有可能导致发热。这种现象称为淋巴瘤综合征，它是由淋巴瘤相关免疫复合物沉积于皮肤、黏膜或其他部位引起的。这些免疫复合物可能引发一种炎症反应，进而影响整个机体，包括中枢神经系统。发热通常是常见症状之一，尤其是在夜间。如果您有任何疑问或需要进一步的帮助，欢迎随时向我咨询。\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "test_df = pd.read_json(test_jsonl_new_path, lines=True)[:3]\n",
    "\n",
    "test_text_list = []\n",
    "\n",
    "for index, row in test_df.iterrows():\n",
    "    instruction = row['instruction']\n",
    "    input_value = row['input']\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": f\"{instruction}\"},\n",
    "        {\"role\": \"user\", \"content\": f\"{input_value}\"}\n",
    "    ]\n",
    "    \n",
    "\n",
    "    response = predict(messages, model, tokenizer)\n",
    "\n",
    "    response_text = f\"\"\"\n",
    "    Question: {input_value}\n",
    "\n",
    "    LLM:{response}\n",
    "    \"\"\"\n",
    "    \n",
    "#     test_text_list.append(swanlab.Text(response_text))\n",
    "    print(response_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bcd35983",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "01240e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False  # we're doing causal LM, not masked LM\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0d53ae37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 241\n",
       "})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ff1dedf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cfee853e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94c74025b5e84146bc26fdbcd59bf900",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating train dataset:   0%|          | 0/2166 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39d4924d3b894923a2401df5077a7e28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating eval dataset:   0%|          | 0/241 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">swanlab</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">:</span> swanlab version <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.6</span>.<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span> is available!  Upgrade: `pip install -U swanlab`\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;34mswanlab\u001b[0m\u001b[1;39m:\u001b[0m swanlab version \u001b[1;36m0.6\u001b[0m.\u001b[1;36m10\u001b[0m is available!  Upgrade: `pip install -U swanlab`\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">swanlab</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">:</span> Tracking run with swanlab version <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.6</span>.<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;34mswanlab\u001b[0m\u001b[1;39m:\u001b[0m Tracking run with swanlab version \u001b[1;36m0.6\u001b[0m.\u001b[1;36m9\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">swanlab</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">:</span> Run data will be saved locally in \n",
       "<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">/home/luke/distributed_machine_learning/Qwen3-Medical-SFT/swanlog/run-20250915_095044-z26s1shexbxf7ji1qoqcd</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;34mswanlab\u001b[0m\u001b[1;39m:\u001b[0m Run data will be saved locally in \n",
       "\u001b[1;35m/home/luke/distributed_machine_learning/Qwen3-Medical-SFT/swanlog/run-20250915_095044-z26s1shexbxf7ji1qoqcd\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">swanlab</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">:</span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\"> </span>👋 Hi <span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">swanluker</span>,welcome to swanlab!\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;34mswanlab\u001b[0m\u001b[1;39m:\u001b[0m\u001b[1;34m \u001b[0m👋 Hi \u001b[1;39mswanluker\u001b[0m,welcome to swanlab!\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">swanlab</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">:</span> Syncing run <span style=\"color: #808000; text-decoration-color: #808000\">qwen3-1.7B-jupyter-trl-v2</span> to the cloud\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;34mswanlab\u001b[0m\u001b[1;39m:\u001b[0m Syncing run \u001b[33mqwen3-1.7B-jupyter-trl-v2\u001b[0m to the cloud\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">swanlab</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">:</span> 🏠 View project at <span style=\"color: #000080; text-decoration-color: #000080; text-decoration: underline\">https://swanlab.cn/@swanluker/qwen3-sft-medical</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;34mswanlab\u001b[0m\u001b[1;39m:\u001b[0m 🏠 View project at \u001b[4;34mhttps://swanlab.cn/@swanluker/qwen3-sft-medical\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">swanlab</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">:</span> 🚀 View run at <span style=\"color: #000080; text-decoration-color: #000080; text-decoration: underline\">https://swanlab.cn/@swanluker/qwen3-sft-medical/runs/z26s1shexbxf7ji1qoqcd</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;34mswanlab\u001b[0m\u001b[1;39m:\u001b[0m 🚀 View run at \u001b[4;34mhttps://swanlab.cn/@swanluker/qwen3-sft-medical/runs/z26s1shexbxf7ji1qoqcd\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<!DOCTYPE html>\n",
       "<html lang=\"en\">\n",
       "<head>\n",
       "    <meta charset=\"UTF-8\">\n",
       "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
       "    <title>Show Iframe</title>\n",
       "    \n",
       "        <script>\n",
       "            function showIframe() {\n",
       "                var iframeHtml = '<iframe src=\"https://swanlab.cn/@swanluker/qwen3-sft-medical/runs/z26s1shexbxf7ji1qoqcd\" width=100% height=\"600\" frameborder=\"no\"></iframe>';\n",
       "                document.getElementById('iframeContainer').innerHTML = iframeHtml;\n",
       "            }\n",
       "        </script>\n",
       "        \n",
       "</head>\n",
       "<body>\n",
       "    <style>\n",
       "        .interactive-button {\n",
       "            display: flex;\n",
       "            align-items: center;\n",
       "            height: 36px;\n",
       "            border: 0px;\n",
       "            background-color: #2c8f63;\n",
       "            color: white;\n",
       "            padding: 10px 20px;\n",
       "            transition: background-color 0.3s, transform 0.2s;\n",
       "        }\n",
       "\n",
       "        .interactive-button:hover {\n",
       "            background-color: #5cab87;\n",
       "            cursor: pointer;\n",
       "        }\n",
       "\n",
       "        .interactive-button:active { background-color: #217952; transform: scale(0.96); } </style> <br> <button \n",
       "        onclick=\"showIframe()\" class=\"interactive-button\"> <svg style=\"height: 16px; margin-right: 8px;\" viewBox=\"0 0 \n",
       "        46 46\" fill=\"none\"> <path d=\"M10.8439 21.1974C10.6414 21.2854 10.4477 21.3925 10.2655 21.5173L10.2069 \n",
       "        21.5652C10.1839 21.58 10.1625 21.5969 10.1429 21.6159C6.29135 24.6118 4.22831 29.4416 5.32646 34.282C5.94656 \n",
       "        37.0577 7.50461 39.5348 9.73801 41.2958C11.9714 43.0568 14.7436 43.994 17.5874 43.9495H18.0219C19.8864 \n",
       "        43.8697 21.7087 43.3694 23.3526 42.486C24.9964 41.6026 26.4193 40.3589 27.5147 38.848C28.61 37.3371 29.3496 \n",
       "        35.598 29.678 33.761C30.0065 31.9239 29.9153 30.0363 29.4112 28.2395C28.9181 26.4723 27.8919 24.8437 26.9937 \n",
       "        23.2551C25.4158 20.4653 23.8343 17.6764 22.2492 14.8884C21.7801 14.0647 21.3057 13.2465 20.8419 \n",
       "        12.4228C20.2315 11.3353 19.2746 10.1519 19.224 8.86183C19.1733 7.57176 20.2235 6.32701 21.5082 \n",
       "        6.07912C23.9284 5.61801 25.0639 8.24078 25.0693 8.23812C25.363 8.94035 25.9123 9.50489 26.6063 \n",
       "        9.81764C27.3002 10.1304 28.087 10.168 28.8077 9.92298C29.5283 9.67791 30.1291 9.1684 30.4885 8.49743C30.8479 \n",
       "        7.82646 30.9392 7.04405 30.7439 6.30835C30.1514 4.37314 28.9133 2.69953 27.2363 1.56656C25.7615 0.511704 \n",
       "        23.9847 -0.0372109 22.1719 0.00195984C20.9049 0.00893199 19.6532 0.27989 18.4967 0.797557C17.3402 1.31522 \n",
       "        16.3043 2.06823 15.4551 3.00856C14.49 4.08707 13.7984 5.38193 13.4389 6.78385C13.0794 8.18576 13.0624 9.6536 \n",
       "        13.3894 11.0635C13.52 11.593 13.6984 12.1095 13.9225 12.6067C14.5595 14.0514 15.4951 15.3681 16.284 \n",
       "        16.7355C17.2525 18.4147 18.2209 20.0948 19.1893 21.7758C20.1578 23.4568 21.1351 25.1449 22.1213 \n",
       "        26.8401C22.9209 28.2421 23.7925 29.4682 23.8805 31.1528C23.9175 32.0513 23.7682 32.9479 23.4419 \n",
       "        33.7859C23.1156 34.6239 22.6194 35.3854 21.9845 36.0223C21.3496 36.6592 20.5897 37.1578 19.7527 \n",
       "        37.4868C18.9157 37.8157 18.0196 37.9678 17.121 37.9336C14.0024 37.7923 11.6488 35.4814 11.1744 32.4588C10.58 \n",
       "        28.6419 13.552 26.5469 13.552 26.5469C14.1782 26.1785 14.6497 25.5955 14.8791 24.906C15.1084 24.2166 15.0801 \n",
       "        23.4673 14.7993 22.7971C14.5186 22.127 14.0044 21.5813 13.3521 21.2611C12.6998 20.941 11.9536 20.8682 11.2517 \n",
       "        21.0561C11.1174 21.0939 10.9856 21.1402 10.8572 21.1947\" fill=\"white\" /> <path d=\"M42.8101 31.5968C42.8109 \n",
       "        30.5198 42.7218 29.4445 42.5435 28.3823C42.2663 26.7069 41.7464 25.0808 41.0002 23.5552C40.5524 22.6463 \n",
       "        39.9874 21.7374 39.1024 21.2417C38.6593 20.9919 38.1589 20.8617 37.6502 20.8639C37.1416 20.8661 36.6423 \n",
       "        21.0006 36.2013 21.2541C35.7604 21.5077 35.393 21.8716 35.1352 22.3101C34.8775 22.7485 34.7382 23.2466 \n",
       "        34.7312 23.7552C34.7072 24.8773 35.3149 25.8875 35.768 26.9217C36.5212 28.6453 36.8623 30.5208 36.7642 \n",
       "        32.3993C36.6661 34.2777 36.1315 36.1075 35.2029 37.7433C35.146 37.8404 35.0952 37.941 35.051 38.0445C34.8623 \n",
       "        38.4842 34.7635 38.9573 34.7605 39.4358C34.7802 40.1222 35.0356 40.7808 35.4835 41.3011C35.9315 41.8214 \n",
       "        36.5449 42.1717 37.2207 42.2932C38.8759 42.589 40.1899 41.347 40.8856 39.9609C42.1643 37.3589 42.823 34.4961 \n",
       "        42.8101 31.5968Z\" fill=\"white\" /> <path d=\"M28.2309 11.8938C28.1761 11.9043 28.1218 11.9176 28.0683 \n",
       "        11.9338C27.9593 11.9642 27.8611 12.0249 27.7851 12.1088C27.7091 12.1928 27.6584 12.2965 27.6389 \n",
       "        12.408C27.6193 12.5195 27.6318 12.6343 27.6748 12.7391C27.7178 12.8438 27.7895 12.9343 27.8818 \n",
       "        12.9999C29.2375 14.0252 30.3809 15.3043 31.2482 16.7662C31.4838 17.1677 31.6888 17.5865 31.8612 \n",
       "        18.0189C32.0052 18.3921 32.1971 18.8799 32.6822 18.8532C33.0607 18.8346 33.2153 18.512 33.3192 \n",
       "        18.1895C33.8137 16.5125 33.9678 14.7534 33.7723 13.0159C33.6331 12.0693 33.4155 11.1359 33.122 \n",
       "        10.2252C33.0775 10.0047 32.9744 9.80029 32.8235 9.6335C32.7273 9.54627 32.6054 9.49262 32.4761 9.4806C32.3468 \n",
       "        9.46859 32.2171 9.49886 32.1065 9.56687C32.0016 9.65188 31.9115 9.75365 31.8399 9.86806C31.3956 10.4658 \n",
       "        30.825 10.9581 30.1687 11.3101C29.8377 11.4861 29.4893 11.6272 29.1292 11.7312C28.828 11.8192 28.5215 11.8325 \n",
       "        28.2309 11.8938Z\" fill=\"white\" /> </svg> Display SwanLab Board </button> <br> <div \n",
       "        id=\"iframeContainer\"></div> </body> </html>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='722' max='722' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [722/722 46:24, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.172700</td>\n",
       "      <td>1.729462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.106700</td>\n",
       "      <td>1.695816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.213200</td>\n",
       "      <td>1.668192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.234800</td>\n",
       "      <td>1.598775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.246700</td>\n",
       "      <td>1.565442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.213800</td>\n",
       "      <td>1.516079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.209000</td>\n",
       "      <td>1.493954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.240900</td>\n",
       "      <td>1.457687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>0.285500</td>\n",
       "      <td>1.438660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.345200</td>\n",
       "      <td>1.423140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>0.159500</td>\n",
       "      <td>1.405936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.259700</td>\n",
       "      <td>1.384612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>0.226500</td>\n",
       "      <td>1.375205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>0.195700</td>\n",
       "      <td>1.361892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.244800</td>\n",
       "      <td>1.346868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.117200</td>\n",
       "      <td>1.335555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>510</td>\n",
       "      <td>0.273400</td>\n",
       "      <td>1.325415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>0.220900</td>\n",
       "      <td>1.318762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>570</td>\n",
       "      <td>0.309600</td>\n",
       "      <td>1.312527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.192200</td>\n",
       "      <td>1.304926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>630</td>\n",
       "      <td>0.284600</td>\n",
       "      <td>1.301968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>660</td>\n",
       "      <td>0.214500</td>\n",
       "      <td>1.299975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>690</td>\n",
       "      <td>0.113000</td>\n",
       "      <td>1.299498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>0.230400</td>\n",
       "      <td>1.299406</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luke/miniconda3/envs/openr1/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'SSLError(SSLEOFError(8, 'EOF occurred in violation of protocol (_ssl.c:992)'))': /api/house/metrics\n",
      "WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'SSLError(SSLEOFError(8, 'EOF occurred in violation of protocol (_ssl.c:992)'))': /api/house/metrics\n",
      "WARNING:urllib3.connectionpool:Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'SSLError(SSLEOFError(8, 'EOF occurred in violation of protocol (_ssl.c:992)'))': /api/house/metrics\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">swanlab</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">:</span> network error, swanlab will resume uploads when the network improves\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;33mswanlab\u001b[0m\u001b[1;39m:\u001b[0m network error, swanlab will resume uploads when the network improves\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luke/miniconda3/envs/openr1/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=722, training_loss=0.20251906229766123, metrics={'train_runtime': 2792.2015, 'train_samples_per_second': 1.551, 'train_steps_per_second': 0.259, 'total_flos': 3.5627337800073216e+16, 'train_loss': 0.20251906229766123})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# args = TrainingArguments(\n",
    "#     output_dir=\"./output/Qwen3-1.7B\",\n",
    "#     per_device_train_batch_size=1,\n",
    "#     per_device_eval_batch_size=1,\n",
    "#     gradient_accumulation_steps=4,\n",
    "#     eval_strategy=\"steps\",\n",
    "#     eval_steps=100,\n",
    "#     logging_steps=10,\n",
    "#     num_train_epochs=2,\n",
    "#     save_steps=400,\n",
    "#     learning_rate=1e-4,\n",
    "#     save_on_each_node=True,\n",
    "#     gradient_checkpointing=True,\n",
    "#     report_to=\"swanlab\",\n",
    "#     run_name=\"qwen3-1.7B\",\n",
    "# )\n",
    "\n",
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     args=args,\n",
    "#     train_dataset=train_dataset,\n",
    "#     eval_dataset=eval_dataset,\n",
    "#     data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer, padding=True),\n",
    "# )\n",
    "\n",
    "\n",
    "\n",
    "args=TrainingArguments(\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=1,\n",
    "    # Use num_train_epochs = 1, warmup_ratio for full training runs!\n",
    "    num_train_epochs=2,\n",
    "    warmup_steps=5,\n",
    "    #max_steps=60,\n",
    "    learning_rate=1e-4,\n",
    "    gradient_checkpointing=True,\n",
    "    logging_steps=1,\n",
    "    \n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=30,\n",
    "\n",
    "    seed=3407,\n",
    "    output_dir=\"outputs\",\n",
    "    report_to=\"swanlab\",\n",
    "    run_name=\"qwen3-1.7B-jupyter-trl-v2\"\n",
    "    \n",
    "    \n",
    "    #    optim=\"adamw_8bit\",\n",
    "    #    weight_decay=0.01,\n",
    "    #    lr_scheduler_type=\"linear\",\n",
    "    #     fp16=not is_bfloat16_supported(),\n",
    "    #     bf16=is_bfloat16_supported(),\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=data_collator,\n",
    "#    dataset_text_field=\"text\",\n",
    "#    max_seq_length=max_seq_length,\n",
    "#    tokenizer=tokenizer,    \n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# 用测试集的前3条，主观看模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4ef8ce26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 2166\n",
       "})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8c451e2e-8542-404c-9eab-8fd610bf8e12",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luke/miniconda3/envs/openr1/lib/python3.11/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Question: 1895年德国物理学教授伦琴的发现对医学影像学的发展有何具体影响？请从技术进步、学科建立和临床应用三个方面进行分析。\n",
      "\n",
      "    LLM:<think>嗯，用户问的是1895年伦琴发现X光对医学影像学的影响，需要从技术进步、学科建立和临床应用三个角度分析。首先，我得回忆一下伦琴是谁，他是德国的物理学家，对吧？他可能是在研究X射线的时候发现了这个现象。\n",
      "\n",
      "首先，技术进步方面，X光的发现肯定是技术上的突破。X射线属于电离辐射，能穿透物质，这比传统的可见光或者紫外线强很多。那为什么这能成为医学影像学的基础呢？可能因为它的穿透性，使得医生可以不用切开病人，就能看到内部结构，比如骨骼或者器官。这应该让诊断更快、更安全，减少了组织损伤，所以技术上是革命性的。\n",
      "\n",
      "然后是学科建立。医学影像学作为一门独立的学科，应该是在伦琴之后才出现的。之前可能医学诊断更多依赖解剖学知识，而X光的出现让影像学成为医学的一部分，比如放射科，专门处理影像检查。可能当时医学界开始将影像学作为诊断的重要手段，而不是仅靠解剖学或病理学。\n",
      "\n",
      "临床应用方面，X光的应用可能促进了后续的诊断技术，比如CT、MRI、超声这些，这些都是在X光的基础上发展起来的。比如CT扫描就是通过多角度的X光成像，然后结合计算机技术分析图像，这比单一的X光片更全面。另外，X光在临床中的普及可能改变了患者对疾病的诊断方式，比如不再需要依赖活检或手术，而是通过影像学直接观察，减少患者的痛苦和风险。\n",
      "\n",
      "不过，用户的问题里可能还希望更深入的分析，比如伦琴如何具体推动这些发展。比如他可能在研究X射线时，意识到它的临床应用潜力，或者他可能在教学或临床实践中应用了这些技术，从而促进了学科的发展。另外，技术进步如何影响后续的发展，比如X光设备的改进，或者成像技术的演变，比如从荧光屏到数字图像技术，这些都是技术进步的延续。\n",
      "\n",
      "还要注意用户可能作为医学生或医学生，想了解医学史，所以需要强调伦琴的贡献不仅停留在技术层面，还影响了学科的建立和临床应用，说明其多方面的影响。可能需要检查是否有其他因素，比如伦琴是否参与了其他医学领域的贡献，或者他的教学和临床实践如何影响了医学影像学的形成。\n",
      "\n",
      "总结下来，回答的结构应该是先分点，每个部分详细说明，用例子和逻辑连接起来，确保覆盖技术、学科、临床应用，并且突出伦琴的直接和间接影响。可能需要确认每个点的具体时间线和影响顺序，比如技术进步是直接结果，学科建立是更长期的影响，临床应用是实际应用的结果。\n",
      "</think> \n",
      " 1895年，德国物理学教授威廉·伦琴在研究X射线时，不仅揭示了X射线的特性，还将其应用于医学领域，这标志着医学影像学的正式建立。X射线技术的进步为医学诊断提供了新的可能性，减少了对活检或手术的依赖，提高了诊断的准确性和安全性。此外，X射线技术的发现促进了医学影像学的学科发展，使得影像学成为医学诊断的重要组成部分。随着X射线技术的进一步应用，它在临床医学中的应用也日益广泛，推动了后续更先进的影像诊断技术的发展。这些变化都极大地改善了患者的治疗体验，提高了医疗诊断的效率和准确性。伦琴的贡献不仅限于技术层面，还为医学影像学的学科建立和临床应用奠定了基础。\n",
      "    \n",
      "\n",
      "    Question: 医生，我最近胃部不适，听说有几种抗溃疡药物可以治疗，您能详细介绍一下这些药物的分类、作用机制以及它们是如何影响胃黏膜的保护与损伤平衡的吗？\n",
      "\n",
      "    LLM:<think>嗯，用户问的是抗溃疡药物的分类、作用机制，还有它们如何影响胃黏膜的保护和损伤平衡。首先，我需要回忆一下抗溃疡药物的基本分类。记得以前学过，抗溃疡药物主要分为几大类，比如H2受体拮抗剂、PPI，还有抗酸药。不过可能还有其他类别，比如抗生素、免疫调节剂之类的，但用户的问题里可能不需要太详细，所以先按主要类别来。\n",
      "\n",
      "接下来是每种药物的作用机制。H2受体拮抗剂比如奥美拉唑，我记得它们通过抑制胃酸分泌，减少对黏膜的损伤。而PPI是质子泵抑制剂，作用更彻底，能抑制H+的分泌，减少胃酸对黏膜的侵蚀。抗酸药比如铝碳酸镁，直接中和胃酸，但可能在长期使用后会有耐药性，所以可能不是首选。\n",
      "\n",
      "然后是关于它们如何影响胃黏膜的保护和损伤平衡。需要解释这些药物如何调节胃酸分泌，从而保护黏膜。比如，H2受体拮抗剂和PPI减少胃酸分泌，从而减少黏膜受损；而抗酸药虽然直接中和，但可能在长期使用后导致黏膜受损，因为胃酸分泌减少后，黏膜可能无法及时修复。\n",
      "\n",
      "用户提到自己有胃部不适，可能需要具体例子，比如提到的奥美拉唑和PPI，以及抗酸药如铝碳酸镁。需要说明这些药物如何通过不同的机制起作用，以及它们的优缺点。比如，H2受体拮抗剂和PPI可能更适合长期使用，而抗酸药可能短期有效但耐药性。\n",
      "\n",
      "可能还需要考虑药物相互作用，比如长期使用抗酸药是否会影响PPI的效果？或者H2受体拮抗剂是否会影响其他药物的代谢？不过用户的问题可能不需要深入这些细节，但答案里已经提到了可能的相互作用，所以需要简要提及。\n",
      "\n",
      "另外，用户可能想知道这些药物如何具体影响胃黏膜的结构和功能，比如改变胃酸分泌的量，减少黏膜接触，从而减少炎症或溃疡。同时，黏膜屏障的修复可能通过药物促进修复过程，比如H2受体拮抗剂可能促进黏膜细胞的修复。\n",
      "\n",
      "需要确保每个药物的作用机制和对黏膜的影响都解释清楚，比如为什么抗酸药虽然能中和胃酸，但可能促进黏膜损伤，因为胃酸分泌减少后，黏膜可能无法及时修复，或者胃酸减少后，黏膜的酸性环境变缓，导致黏膜损伤。\n",
      "\n",
      "可能还要比较不同药物的优缺点，比如H2受体拮抗剂和PPI在胃酸分泌抑制上的效果差异，或者抗酸药的短期效果。这样用户能根据自己的情况选择药物。\n",
      "\n",
      "总结下来，结构应该是先分类，再分别解释每种药物的作用机制，然后说明它们如何影响黏膜的保护和损伤平衡，最后可能提到药物相互作用和长期使用的影响。确保语言通俗易懂，避免过于专业的术语，但用户是医生，可能需要一定的专业性，所以需要平衡。\n",
      "</think> \n",
      " 当然可以。抗溃疡药物主要分为几类，包括H2受体拮抗剂、PPI和抗酸药。H2受体拮抗剂如奥美拉唑，它们通过减少胃酸分泌来保护胃黏膜，防止胃酸对黏膜的侵蚀。PPI，如质子泵抑制剂，作用于胃黏膜上的H+泵，进一步减少胃酸分泌，增强黏膜的保护作用。抗酸药如铝碳酸镁，虽然能快速中和胃酸，但可能对胃黏膜的保护作用有限，因为长期使用抗酸药可能会导致黏膜的损伤，尤其是在胃酸分泌减少的情况下。每种药物都有其特点和适应症，医生会根据您的具体情况来选择最合适的药物。此外，长期使用抗酸药可能会引起胃酸分泌减少，这可能会影响黏膜的保护作用，因此需要谨慎使用。希望这些信息对您有所帮助。如果您有任何疑问，欢迎随时向我提问。\n",
      "    \n",
      "\n",
      "    Question: 我最近被诊断出患有淋巴瘤，医生提到这可能导致发热。请问这是由于淋巴瘤组织的坏死和细胞破坏引起的吗？如果是，具体机制是什么？\n",
      "\n",
      "    LLM:<think>嗯，用户问的是淋巴瘤导致发热的具体机制，还提到自己被诊断出淋巴瘤，医生说可能发热。首先，我需要回忆一下淋巴瘤的基本知识。淋巴瘤是一种起源于淋巴细胞的恶性肿瘤，通常发生在年轻人身上，尤其是15到30岁之间。发热是淋巴瘤常见的症状之一，但具体原因是什么呢？\n",
      "\n",
      "首先，用户提到的坏死和细胞破坏应该是指淋巴瘤细胞在肿瘤生长过程中，可能因为增殖过度导致坏死。坏死会导致细胞内容物泄漏，可能刺激免疫系统。然后，细胞破坏可能指的是肿瘤细胞本身被破坏，释放的物质进入体内，引发炎症反应。\n",
      "\n",
      "接下来，我需要详细解释坏死的具体机制。坏死可能发生在肿瘤细胞快速增殖时，导致细胞结构破坏，比如细胞膜破裂，细胞内容物如细胞质、细胞核等被释放。这些物质包括细胞内的物质，比如细胞内的物质可能被释放到血液中，进而刺激免疫系统。\n",
      "\n",
      "然后，细胞破坏可能是指肿瘤细胞被破坏，比如凋亡或坏死后的细胞碎片进入血液。这些细胞碎片可能含有细胞内的物质，比如细胞质中的细胞器，或者细胞核中的DNA等，这些物质被释放到血液中，引发炎症反应。\n",
      "\n",
      "炎症反应是发热的核心机制。当这些物质被释放到血液中后，会被免疫系统识别为异物，引发炎症反应。炎症反应会激活免疫细胞，比如巨噬细胞、中性粒细胞等，导致白细胞和淋巴细胞的活化，释放更多的炎症介质，比如内源性致热原（IL-1、IL-2、TNF-α等），这些物质刺激血管收缩，促进血管通透性，导致更多白蛋白进入血管，形成血栓，从而引起发热。\n",
      "\n",
      "另外，发热是免疫系统的一种防御反应，但在这个情况下，可能由于过度激活导致发热。需要区分发热是正常的免疫反应还是过度反应，但淋巴瘤患者可能因为免疫系统的异常激活而出现发热。\n",
      "\n",
      "可能用户还想知道为什么淋巴瘤患者更容易发热，或者发热对治疗的影响。但根据问题，重点在机制，所以需要集中在坏死和细胞破坏如何引发炎症反应。\n",
      "\n",
      "需要确认坏死是否真的是淋巴瘤的主要机制，以及炎症反应是否是主要途径。可能还要考虑其他可能的机制，比如细胞因子的释放，或者免疫细胞的活化，但根据答案部分，重点在炎症反应。\n",
      "\n",
      "总结的话，坏死和细胞破坏导致细胞内容物释放，引发炎症反应，导致发热。需要将这些点有条理地组织起来，确保用户理解每个环节的关系。\n",
      "</think> \n",
      " 是的，淋巴瘤可能导致发热。这种发热主要是由于淋巴瘤组织的坏死和细胞破坏引起的。当淋巴瘤细胞增殖过度时，它们可能会发生坏死，导致细胞内容物的释放，这些物质可以刺激免疫系统，引发炎症反应。炎症反应是发热的主要机制，当免疫系统被激活时，它会通过释放炎症介质来应对刺激，这可能导致发热。如果发热过高，可能会引起全身性的炎症反应综合征，进而影响患者的健康。建议您密切监测发热情况，并及时与医生沟通，以便采取相应的治疗措施。同时，保持良好的心态对疾病的康复也非常重要，可以适当进行一些放松和愉悦的活动。希望这些信息对您有所帮助。\n",
      "    \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">swanlab</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">:</span> 🏠 View project at <span style=\"color: #000080; text-decoration-color: #000080; text-decoration: underline\">https://swanlab.cn/@swanluker/qwen3-sft-medical</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;34mswanlab\u001b[0m\u001b[1;39m:\u001b[0m 🏠 View project at \u001b[4;34mhttps://swanlab.cn/@swanluker/qwen3-sft-medical\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">swanlab</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">:</span> 🚀 View run at <span style=\"color: #000080; text-decoration-color: #000080; text-decoration: underline\">https://swanlab.cn/@swanluker/qwen3-sft-medical/runs/z26s1shexbxf7ji1qoqcd</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;34mswanlab\u001b[0m\u001b[1;39m:\u001b[0m 🚀 View run at \u001b[4;34mhttps://swanlab.cn/@swanluker/qwen3-sft-medical/runs/z26s1shexbxf7ji1qoqcd\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "test_df = pd.read_json(test_jsonl_new_path, lines=True)[:3]\n",
    "\n",
    "test_text_list = []\n",
    "\n",
    "for index, row in test_df.iterrows():\n",
    "    instruction = row['instruction']\n",
    "    input_value = row['input']\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": f\"{instruction}\"},\n",
    "        {\"role\": \"user\", \"content\": f\"{input_value}\"}\n",
    "    ]\n",
    "\n",
    "    response = predict(messages, model, tokenizer)\n",
    "\n",
    "    response_text = f\"\"\"\n",
    "    Question: {input_value}\n",
    "\n",
    "    LLM:{response}\n",
    "    \"\"\"\n",
    "    \n",
    "    test_text_list.append(swanlab.Text(response_text))\n",
    "    print(response_text)\n",
    "\n",
    "swanlab.log({\"Prediction\": test_text_list})\n",
    "\n",
    "swanlab.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2df6f8c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
